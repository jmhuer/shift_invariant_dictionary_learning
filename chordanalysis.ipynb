{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM6ZtjBgPA4dlCOSHrSnV2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/shift_invariant_dictionary_learning/blob/main/chordanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "d761bc94-1d14-4416-e24e-794791b32d70"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/jmhuer/ModularSparseAutoencoder\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "!git clone https://github.com/jmhuer/HT\n",
        "!git clone https://github.com/Tsung-Ping/functional-harmony\n",
        "# %cd /content/POP909-Dataset/data_process\n",
        "!pip install libfmp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=59b7f9bf648b7b88f6dae57bee6469579b1108f84e2f2450d25a70ce7eb9c78b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'ModularSparseAutoencoder'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 25 (delta 10), reused 10 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 23.98 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Cloning into 'HT'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 23 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (23/23), done.\n",
            "Cloning into 'functional-harmony'...\n",
            "remote: Enumerating objects: 460, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 460 (delta 13), reused 49 (delta 12), pack-reused 396\u001b[K\n",
            "Receiving objects: 100% (460/460), 2.50 MiB | 11.90 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "Collecting libfmp\n",
            "  Downloading libfmp-1.2.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa<1.0.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.8.1)\n",
            "Collecting pysoundfile<1.0.0,>=0.9.0\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numba<1.0.0,>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.51.2)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.1.5)\n",
            "Collecting ipython<8.0.0,>=7.8.0\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 77.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.19.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (3.2.2)\n",
            "Collecting music21<6.0.0,>=5.7.0\n",
            "  Downloading music21-5.7.2.tar.gz (18.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.5 MB 339 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pretty-midi<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.2.9)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.4.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (5.1.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.18.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (57.4.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.8.0->libfmp) (0.8.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (21.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.22.2.post1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (2.1.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.1.0->libfmp) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1.0.0,>=0.51.0->libfmp) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->libfmp) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.8.0->libfmp) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.4.4)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty-midi<1.0.0,>=0.2.0->libfmp) (1.2.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.8.0->libfmp) (0.2.5)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile<1.0.0,>=0.9.0->libfmp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile<1.0.0,>=0.9.0->libfmp) (2.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (3.0.4)\n",
            "Building wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-5.7.2-py3-none-any.whl size=22024624 sha256=e6dac8bf4265f9a0e37a413bc200978a31ddb069239be15cd68a3bfb1f7da7da\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cb/ae/fd264ebf1e9cf01c15576ee4c128f1bfd907a120c0a7a5b542\n",
            "Successfully built music21\n",
            "Installing collected packages: prompt-toolkit, pysoundfile, music21, ipython, libfmp\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: music21\n",
            "    Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.28.0 libfmp-1.2.1 music21-5.7.2 prompt-toolkit-3.0.20 pysoundfile-0.9.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3uwo-E8NFshn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1fdb32-a678-45f7-f4c4-69505200fab8"
      },
      "source": [
        "#@title Pytorch for DL\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "\n",
        "def get_model_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BA7MRaf3nrC"
      },
      "source": [
        "# This preprocesses data and creates pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dDuXiM626TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49399202-9de8-4cc8-a403-b117f4d4e064"
      },
      "source": [
        "\n",
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "\n",
        "\n",
        "# Chord symbol recognition\n",
        "# train_BTC() # Bi-directional Transformer for Chord Recognition\n",
        "# train_CRNN() # Convolutional Recurrent Neural Network\n",
        "root_dict = {'C': 0, 'C+': 1, 'D': 2, 'D+': 3, 'E': 4, 'F': 5, 'F+': 6, 'G': 7, 'G+': 8, 'A': 9, 'A+': 10, 'B': 11, 'pad': 12}\n",
        "tquality_dict = {'M': 0, 'm': 1, 'O': 2, 'pad': 3}  # 'O' stands for 'others'\n",
        "n_chord_classes = 24 + 1  # 24 major-minor modes plus 1 others\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "hyperparameters = namedtuple('hyperparameters',\n",
        "                              ['dataset',\n",
        "                              'test_set_id',\n",
        "                              'graph_location',\n",
        "                              'n_root_classes',\n",
        "                              'n_tquality_classes',\n",
        "                              'n_chord_classes',\n",
        "                              'n_steps',\n",
        "                              'input_embed_size',\n",
        "                              'n_layers',\n",
        "                              'n_heads',\n",
        "                              'train_sequence_with_overlap',\n",
        "                              'initial_learning_rate',\n",
        "                              'drop',\n",
        "                              'n_batches',\n",
        "                              'n_training_steps',\n",
        "                              'n_in_succession',\n",
        "                              'annealing_rate'])\n",
        "\n",
        "hp = hyperparameters(dataset='/content/', # {'BPS_FH', 'Preludes'}\n",
        "                      test_set_id=1, # {1, 2, 3, 4}\n",
        "                      graph_location='model',\n",
        "                      n_root_classes=len(root_dict.keys()),\n",
        "                      n_tquality_classes=len(tquality_dict.keys()),\n",
        "                      n_chord_classes=n_chord_classes,\n",
        "                      n_steps=128,\n",
        "                      input_embed_size=128,\n",
        "                      n_layers=2,\n",
        "                      n_heads=4,\n",
        "                      train_sequence_with_overlap=True,\n",
        "                      initial_learning_rate=1e-4,\n",
        "                      drop=0.1,\n",
        "                      n_batches=40,\n",
        "                      n_training_steps=100000,\n",
        "                      n_in_succession=10,\n",
        "                      annealing_rate=1.1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km3TD1zn3j3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6582181-69ac-46f0-e1c9-47810613738b"
      },
      "source": [
        "from HT.BPS_FH_preprocessing import main\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: load note data ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrXxS30i3ygJ"
      },
      "source": [
        "# Make pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRQ7nWU1zIZ"
      },
      "source": [
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "from HT.BPS_FH_preprocessing import main\n",
        "from HT.chord_symbol_recognition import load_data_symbol\n",
        "#\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "class BPS(Dataset):\n",
        "    def __init__(self, data):\n",
        "        \"todo: transpose\"\n",
        "        self.all_data = []\n",
        "        temp = []\n",
        "        self.label = []\n",
        "        temp2 = []\n",
        "        self.chord_change = []\n",
        "        temp3 = []      \n",
        "        for pi,chord, cc in zip(data[\"pianoroll\"],data[\"tchord\"],data['label']['chord_change']):\n",
        "            temp.append(torch.tensor(pi).double())\n",
        "            temp2.append(torch.tensor(chord).double())\n",
        "            temp3.append(torch.tensor(cc).double())\n",
        "        self.all_data = torch.cat(temp)\n",
        "        self.label = torch.cat(temp2)\n",
        "        self.chord_change = torch.cat(temp3)\n",
        "        print(\"data\" , self.all_data.shape)\n",
        "        print(\"label\" , self.label.shape)\n",
        "        ##time.sleep(120)\n",
        "        self.transform = self.make_transform()\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "    def __getitem__(self, idx):\n",
        "        piano_roll_slice = self.all_data[idx,:]\n",
        "        label_slice = self.label[idx]\n",
        "        chord_change_slice = self.chord_change[idx]\n",
        "        tranformed_piano_roll_slice = self.transform[\"norm\"](piano_roll_slice[None][None])[0,0,:]\n",
        "        #print(tranformed_piano_roll_slice.shape)\n",
        "        #print(\"label_slice\", label_slice)\n",
        "        return tranformed_piano_roll_slice, label_slice, chord_change_slice\n",
        "    def make_transform(self):\n",
        "        mean = self.all_data.mean()\n",
        "        std = self.all_data.std() * 2\n",
        "        # print(\"mean \", mean.shape)\n",
        "        # print(\"std \", std.shape)\n",
        "        tensor_transform = {\n",
        "          'norm':\n",
        "              transforms.Compose([\n",
        "                  transforms.Normalize([mean], [std])  # Imagenet standards\n",
        "              ]),\n",
        "          \"inverse_norm\":\n",
        "                transforms.Normalize(\n",
        "                  mean= [-m/s for m, s in zip([mean], [std])],\n",
        "                  std= [1/s for s in [std]]\n",
        "                )\n",
        "            }\n",
        "        return tensor_transform\n",
        "\n",
        "\n",
        "def load_data(train_data, test_data,  num_workers=0, batch_size=16, random_seed = 40):\n",
        "    '''\n",
        "    this data loading proccedure assumes dataset/train/ dataset/val/ folders\n",
        "    also assumes transform dictionary with train and val\n",
        "    '''\n",
        "    dataset_train = BPS(train_data) \n",
        "    dataset_val = BPS(test_data) \n",
        "\n",
        "    print(\"Size of train dataset: \",len(dataset_train))\n",
        "    print(\"Size of val dataset: \",len(dataset_val))\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(dataset_train, batch_size=batch_size, shuffle=False, drop_last=True),\n",
        "        'val': DataLoader(dataset_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    }\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "\n",
        "##only tensor transforms\n",
        "\n",
        "\n",
        "\n",
        "train_data, test_data = load_data_symbol(dir=hp.dataset + 'BPS_FH_preprocessed_data_MIREX_Mm.pickle', test_set_id=hp.test_set_id, sequence_with_overlap=hp.train_sequence_with_overlap)\n",
        "print(\"load_data_symbol train_data size: {}\".format(train_data[\"tchord\"].shape))\n",
        "\n",
        "\n",
        "\n",
        "dataset = load_data(train_data, test_data, batch_size=16)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9CkJV5MthTU"
      },
      "source": [
        "#play example from BPS dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=8, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxlEUDlC_5y"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i,ba in enumerate(dataset[\"val\"]): \n",
        "    ##print(ba[0].shape)\n",
        "    if i < index: \n",
        "        continue\n",
        "    listen.append(tr(ba[0][:,None,None,:]))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll)\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eViZSBMro3iZ"
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Architecture Flags\n",
        "    parser.add_argument('--intermediate_dim', type=int, default=32)\n",
        "    parser.add_argument('--stripe_dim', type=int, default=32)\n",
        "    parser.add_argument('--num_stripes', type=int, default=10)\n",
        "    parser.add_argument('--num_active_neurons', type=int, default=4)\n",
        "    parser.add_argument('--num_active_stripes', type=int, default=3)\n",
        "    parser.add_argument('--layer_sparsity_mode', type=str, default='lifetime')  # Set to none, ordinary, boosted, or lifetime.\n",
        "    parser.add_argument('--stripe_sparsity_mode', type=str, default='routing')  # Set to none, ordinary, or routing.\n",
        "    parser.add_argument('--distort_prob', type=float, default=.01)  # Probability of stripe sparsity mask bits randomly flipping.\n",
        "    parser.add_argument('--distort_prob_decay', type=float, default=.025)  # Lowers distort_prob by this amount every epoch.\n",
        "\n",
        "    # Boosting Flags - Only necessary when layer_sparsity_mode is set to boosted.\n",
        "    parser.add_argument('--alpha', type=float, default=.8)\n",
        "    parser.add_argument('--beta', type=float, default=1.2)\n",
        "\n",
        "    # Routing Flags - Only necessary when stripe_sparsity_mode is set to routing.\n",
        "    parser.add_argument('--routing_l1_regularization', type=float, default=0.1)\n",
        "    parser.add_argument('--log_average_routing_scores', type=bool, default=True)\n",
        "\n",
        "    # Lifetime Stripe Flag - Only necessary when stripe_sparsity_mode is set to lifetime.\n",
        "    # Within a stripe, this specifies the proportion of samples that may activate the stripe.\n",
        "    parser.add_argument('--active_stripes_per_batch', type=float, default=4)\n",
        "\n",
        "    # Training Flags\n",
        "    parser.add_argument('--lr', type=float, default=.0001)\n",
        "    parser.add_argument('--momentum', type=float, default=.1)\n",
        "    parser.add_argument('--num_epochs', type=int, default=4)\n",
        "    parser.add_argument('--batch_size', type=int, default=8)\n",
        "    parser.add_argument('--data_path', type=str, default='data.csv')\n",
        "    parser.add_argument('--log_path', type=str, default='logs')\n",
        "    parser.add_argument('--log_class_specific_losses', type=bool, default=False)\n",
        "    parser.add_argument('--log_average_activations', type=bool, default=True)\n",
        "    parser.add_argument('--use_cuda_if_available', type=bool, default=True)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return vars(args)\n",
        "args = get_args()\n",
        "print(args[\"intermediate_dim\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEMdCVNo3qc"
      },
      "source": [
        "from ModularSparseAutoencoder.model import Net\n",
        "from ModularSparseAutoencoder.train import train\n",
        "\n",
        "\n",
        "num_stripes = args['num_stripes']\n",
        "num_epochs = args['num_epochs']\n",
        "batch_size = args['batch_size']\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "net = Net(args['intermediate_dim'],\n",
        "          args['stripe_dim'],\n",
        "          args['num_stripes'],\n",
        "          args['num_active_neurons'],\n",
        "          args['num_active_stripes'],\n",
        "          args['layer_sparsity_mode'],\n",
        "          args['stripe_sparsity_mode'],\n",
        "          args['distort_prob'],\n",
        "          args['alpha'],\n",
        "          args['beta'],\n",
        "          args['active_stripes_per_batch'],\n",
        "          device).double().to(device)\n",
        "\n",
        "# net.load_state_dict(torch.load(\"/content/net.pth\"))\n",
        "\n",
        "criterion = nn.MSELoss().to(device)\n",
        "# optimizer = optim.SGD(net.parameters(),\n",
        "#                       lr=args['lr'],\n",
        "#                       momentum=args['momentum'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'],  \n",
        "                       betas=(0.8, 0.999), eps=1e-08, weight_decay=0, amsgrad=True) ##this has weight decay just like you implemented\n",
        "\n",
        "timestamp = str(datetime.datetime.now()).replace(' ', '_')\n",
        "root_path = os.path.join(args['log_path'],\n",
        "                          args['layer_sparsity_mode'],\n",
        "                          args['stripe_sparsity_mode'],\n",
        "                          timestamp)\n",
        "print(f'Logging results to path:  {root_path}')\n",
        "\n",
        "distort_prob_decay = args['distort_prob_decay']\n",
        "routing_l1_regularization = (args['routing_l1_regularization'] if args['stripe_sparsity_mode'] == 'routing' else 0)\n",
        "log_class_specific_losses = args['log_class_specific_losses']\n",
        "should_log_average_routing_scores = (\n",
        "            args['stripe_sparsity_mode'] == 'routing' and args['log_average_routing_scores'])\n",
        "\n",
        "train(net,\n",
        "      criterion,\n",
        "      optimizer,\n",
        "      root_path,\n",
        "      dataset,\n",
        "      num_stripes,\n",
        "      num_epochs,\n",
        "      distort_prob_decay,\n",
        "      routing_l1_regularization,\n",
        "      log_class_specific_losses,\n",
        "      should_log_average_routing_scores)\n",
        "  \n",
        "# if args['log_average_activations']:\n",
        "#     average_activations_path = os.path.join(root_path, 'average_activations.json')\n",
        "#     with open(average_activations_path, 'w') as f:\n",
        "#         average_activations = net.get_average_activations(X_test, Y_test, device=device).tolist()\n",
        "#         f.write(json.dumps(average_activations))\n",
        "\n",
        "flag_values_path = os.path.join(root_path, 'experiment_config.json')\n",
        "with open(flag_values_path, 'w') as f:\n",
        "    f.write(json.dumps(args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MpTvdvFFksg"
      },
      "source": [
        "#evaluate "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbLe78prk0XS"
      },
      "source": [
        "#re init data for batch 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0cjGZwwFj1k"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i, ba in enumerate(dataset[\"val\"]): \n",
        "    if i < index: \n",
        "        continue\n",
        "    x_var = ba[0].to(net.device)\n",
        "    xpred_var = net(x_var)[:,None,None,:]\n",
        "    listen.append(tr(xpred_var))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        piano_roll = (piano_roll>=1) \n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll.cpu().detach().numpy())\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN2XoiSO2wbm"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mge7u8T0hoZE"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koRxRNdM4jTW"
      },
      "source": [
        "#load data with batch size to be used in transformer \n",
        "traintr = dataset[\"train\"].dataset.transform[\"inverse_norm\"]\n",
        "valtr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "\n",
        "\n",
        "preprocessed_train_data = []\n",
        "preprocessed_train_labeldata = []\n",
        "preprocessed_train_cc = []\n",
        "\n",
        "for i, batch in enumerate(dataset[\"train\"]): \n",
        "    x_var = batch[0].to(net.device)\n",
        "    x_var = x_var.to(net.device)\n",
        "    xpred_var = net(x_var)\n",
        "    # xpred_var = x_var\n",
        "    preprocessed_train_data.append(traintr(xpred_var[:,None,None,:])[:,0,0,:].cpu().detach().numpy())\n",
        "    preprocessed_train_labeldata.append(batch[1].cpu().detach().numpy())\n",
        "    preprocessed_train_cc.append(batch[2].cpu().detach().numpy())\n",
        "\n",
        "preprocessed_test_data = []\n",
        "preprocessed_test_labeldata = []\n",
        "preprocessed_test_cc = []\n",
        "\n",
        "for i, batch in enumerate(dataset[\"val\"]): \n",
        "    x_var = batch[0].to(net.device)\n",
        "    x_var = x_var.to(net.device)\n",
        "    xpred_var = net(x_var)\n",
        "    # xpred_var = x_var\n",
        "    preprocessed_test_data.append(valtr(xpred_var[:,None,None,:])[:,0,0,:].cpu().detach().numpy())\n",
        "    preprocessed_test_labeldata.append(batch[1].cpu().detach().numpy())\n",
        "    preprocessed_test_cc.append(batch[2].cpu().detach().numpy())\n",
        "\n",
        "\n",
        "preprocessed_train_data = np.array(preprocessed_train_data)\n",
        "preprocessed_train_labeldata= np.array(preprocessed_train_labeldata)\n",
        "preprocessed_train_cc = np.array(preprocessed_train_cc)\n",
        "\n",
        "preprocessed_test_data = np.array(preprocessed_test_data)\n",
        "preprocessed_test_labeldata = np.array(preprocessed_test_labeldata)\n",
        "preprocessed_test_cc = np.array(preprocessed_test_cc)\n",
        "\n",
        "print(preprocessed_train_data.shape)\n",
        "print(preprocessed_test_data.shape)\n",
        "\n",
        "\n",
        "print(preprocessed_train_labeldata.shape)\n",
        "print(preprocessed_test_labeldata.shape)\n",
        "\n",
        "print(preprocessed_train_cc.shape)\n",
        "print(preprocessed_test_cc.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o77itkZhkzv7"
      },
      "source": [
        "import gc\n",
        "del dataset\n",
        "gc.collect()\n",
        "\n",
        "train_data2 = {}\n",
        "test_data2 = {}\n",
        "train_data2['label'] = {}\n",
        "test_data2['label'] = {}\n",
        "\n",
        "train_data2['label']['chord_change']= preprocessed_train_cc\n",
        "train_data2[\"pianoroll\"] =  preprocessed_train_data  \n",
        "train_data2[\"tchord\"] =  preprocessed_train_labeldata  \n",
        "\n",
        "test_data2['label']['chord_change'] =  preprocessed_test_cc\n",
        "test_data2[\"pianoroll\"] =  preprocessed_test_data  \n",
        "test_data2[\"tchord\"] =  preprocessed_test_labeldata  \n",
        "\n",
        "dataset = load_data(train_data2, test_data2, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OQ8RIN7qZ1G"
      },
      "source": [
        "#load data with batch size to be used in transformer \n",
        "traintr = dataset[\"train\"].dataset.transform[\"inverse_norm\"]\n",
        "valtr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "\n",
        "#all\n",
        "how_many_batches_train = 54320 \n",
        "how_many_batches_test = 294\n",
        "\n",
        "preprocessed_train_data = []\n",
        "preprocessed_train_labeldata = []\n",
        "preprocessed_train_cc = []\n",
        "\n",
        "for i, batch in enumerate(dataset[\"train\"]): \n",
        "    x_var = batch[0]\n",
        "    x_var = x_var\n",
        "    xpred_var = x_var\n",
        "    preprocessed_train_data.append(traintr(xpred_var[:,None,None,:])[:,0,0,:].detach().numpy())\n",
        "    preprocessed_train_labeldata.append(batch[1].detach().numpy())\n",
        "    preprocessed_train_cc.append(batch[2].detach().numpy())\n",
        "    if len(preprocessed_train_data)==how_many_batches_train: break\n",
        "\n",
        "\n",
        "\n",
        "preprocessed_test_data = []\n",
        "preprocessed_test_labeldata = []\n",
        "preprocessed_test_cc = []\n",
        "\n",
        "for i, batch in enumerate(dataset[\"val\"]): \n",
        "    x_var = batch[0]\n",
        "    x_var = x_var\n",
        "    xpred_var = x_var\n",
        "    preprocessed_test_data.append(valtr(xpred_var[:,None,None,:])[:,0,0,:].detach().numpy())\n",
        "    preprocessed_test_labeldata.append(batch[1].detach().numpy())\n",
        "    preprocessed_test_cc.append(batch[2].detach().numpy())\n",
        "    if len(preprocessed_test_data)==how_many_batches_test: break\n",
        "\n",
        "\n",
        "preprocessed_train_data = np.array(preprocessed_train_data)\n",
        "preprocessed_train_labeldata= np.array(preprocessed_train_labeldata)\n",
        "preprocessed_train_cc = np.array(preprocessed_train_cc)\n",
        "\n",
        "preprocessed_test_data = np.array(preprocessed_test_data)\n",
        "preprocessed_test_labeldata = np.array(preprocessed_test_labeldata)\n",
        "preprocessed_test_cc = np.array(preprocessed_test_cc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(preprocessed_train_data.shape)\n",
        "print(preprocessed_test_data.shape)\n",
        "\n",
        "\n",
        "print(preprocessed_train_labeldata.shape)\n",
        "print(preprocessed_test_labeldata.shape)\n",
        "\n",
        "print(preprocessed_train_cc.shape)\n",
        "print(preprocessed_test_cc.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAnetb0j3spB"
      },
      "source": [
        "# this trains Harmony transformer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OxcTDxCWMTg"
      },
      "source": [
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "# Chord symbol recognition\n",
        "# train_BTC() # Bi-directional Transformer for Chord Recognition\n",
        "# train_CRNN() # Convolutional Recurrent Neural Network\n",
        "train_data3 = {}\n",
        "test_data3 = {}\n",
        "train_data3['label'] = {}\n",
        "test_data3['label'] = {}\n",
        "\n",
        "train_data3['label']['chord_change']= preprocessed_train_cc\n",
        "train_data3[\"len\"] = np.array([128 for i in range(how_many_batches_train)])\n",
        "train_data3[\"pianoroll\"] =  preprocessed_train_data  \n",
        "train_data3[\"tchord\"] =  preprocessed_train_labeldata  \n",
        "train_data3['root'] = np.zeros((how_many_batches_train,128))\n",
        "train_data3['tquality'] =  np.zeros((how_many_batches_train,128))\n",
        "\n",
        "test_data3['label']['chord_change'] =  preprocessed_test_cc\n",
        "test_data3[\"len\"] = np.array([128 for i in range(how_many_batches_test)])\n",
        "test_data3[\"pianoroll\"] =  preprocessed_test_data  \n",
        "test_data3[\"tchord\"] =  preprocessed_test_labeldata  \n",
        "test_data3['root'] =  np.zeros((how_many_batches_test,128))\n",
        "test_data3['tquality'] =  np.zeros((how_many_batches_test,128))\n",
        "\n",
        "# import os\n",
        "train_HT(hp, train_data3, test_data3)\n",
        "\n",
        "# print(test_data[\"pianoroll\"].shape, test_data2[\"pianoroll\"].shape)\n",
        "# print(test_data['label']['chord_change'].shape, test_data2['label']['chord_change'].shape)\n",
        "# print(test_data[\"tchord\"].shape, test_data2[\"tchord\"].shape)\n",
        "# print(test_data[\"len\"].shape, test_data2[\"len\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}