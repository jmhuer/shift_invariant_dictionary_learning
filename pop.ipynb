{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMXxoS/qfZz8Ap21yKdmwK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/shift_invariant_dictionary_learning/blob/main/pop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "276e31a4-e15b-4171-b99e-86fe5ead96d6"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "%cd /content/POP909-Dataset/data_process\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=949c75f7dda18e7a8baf1ad4ba85b386e853242a6d690f01baac34f2aabf5dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 29.62 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/POP909-Dataset/data_process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3uwo-E8NFshn"
      },
      "source": [
        "#@title Pytorch for DL\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "\n",
        "def get_model_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePLQyAsOyc60",
        "outputId": "9545de11-9b15-4956-eaff-2de4d0d6e9c6"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "from processor import MidiEventProcessor\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "total = 0\n",
        "def preprocess_midi(path):\n",
        "    global total\n",
        "    data = pretty_midi.PrettyMIDI(path)\n",
        "    main_notes = []\n",
        "    acc_notes = []\n",
        "    for ins in data.instruments:\n",
        "        acc_notes.extend(ins.notes)\n",
        "    for i in range(len(main_notes)):\n",
        "        main_notes[i].start = round(main_notes[i].start,2)\n",
        "        main_notes[i].end = round(main_notes[i].end,2)\n",
        "    for i in range(len(acc_notes)):\n",
        "        acc_notes[i].start = round(acc_notes[i].start,2)\n",
        "        acc_notes[i].end = round(acc_notes[i].end,2)\n",
        "    main_notes.sort(key = lambda x:x.start)\n",
        "    acc_notes.sort(key = lambda x:x.start)\n",
        "\n",
        "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
        "    piano = pretty_midi.Instrument(program=piano_program)\n",
        "    piano.notes.extend(acc_notes)\n",
        "    # score = prettyn\n",
        "    # mpr = MidiEventProcessor()\n",
        "    # repr_seq = mpr.encode([main_notes, acc_notes])\n",
        "    total += 1\n",
        "    return piano.get_piano_roll()\n",
        "\n",
        "def preprocess_pop909(midi_root, save_dir):\n",
        "    save_py = []\n",
        "    midi_paths = [d for d in os.listdir(midi_root)] #not index\n",
        "    i = 0\n",
        "    out_fmt = '{}-{}.data'\n",
        "    for path in midi_paths:\n",
        "        if \".\" not in path:\n",
        "          # print(' ', end='[{}]'.format(path), flush=True)\n",
        "          filename = midi_root + path  +\"/\"+path[0:3] + \".mid\"\n",
        "          try:\n",
        "              data = torch.tensor(preprocess_midi(filename))\n",
        "              print(data.shape)\n",
        "          except KeyboardInterrupt:\n",
        "              print(' Abort')\n",
        "              return\n",
        "          except EOFError:\n",
        "              print('EOF Error')\n",
        "              return\n",
        "          save_py.append(data)\n",
        "    return save_py\n",
        "     \n",
        "    \n",
        "# replace the folder with your POP909 data folder\n",
        "train_dataset = preprocess_pop909(\"../POP909/\",\"midi_data/\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 26426])\n",
            "torch.Size([128, 16903])\n",
            "torch.Size([128, 21515])\n",
            "torch.Size([128, 24289])\n",
            "torch.Size([128, 27367])\n",
            "torch.Size([128, 23618])\n",
            "torch.Size([128, 24140])\n",
            "torch.Size([128, 25326])\n",
            "torch.Size([128, 25852])\n",
            "torch.Size([128, 27320])\n",
            "torch.Size([128, 28448])\n",
            "torch.Size([128, 29813])\n",
            "torch.Size([128, 22922])\n",
            "torch.Size([128, 23535])\n",
            "torch.Size([128, 24609])\n",
            "torch.Size([128, 25240])\n",
            "torch.Size([128, 21341])\n",
            "torch.Size([128, 25675])\n",
            "torch.Size([128, 23753])\n",
            "torch.Size([128, 24925])\n",
            "torch.Size([128, 25786])\n",
            "torch.Size([128, 27589])\n",
            "torch.Size([128, 22386])\n",
            "torch.Size([128, 28606])\n",
            "torch.Size([128, 25860])\n",
            "torch.Size([128, 25355])\n",
            "torch.Size([128, 32616])\n",
            "torch.Size([128, 31954])\n",
            "torch.Size([128, 27881])\n",
            "torch.Size([128, 22469])\n",
            "torch.Size([128, 22353])\n",
            "torch.Size([128, 25545])\n",
            "torch.Size([128, 27894])\n",
            "torch.Size([128, 25400])\n",
            "torch.Size([128, 30304])\n",
            "torch.Size([128, 25229])\n",
            "torch.Size([128, 21738])\n",
            "torch.Size([128, 23334])\n",
            "torch.Size([128, 26036])\n",
            "torch.Size([128, 14997])\n",
            "torch.Size([128, 25775])\n",
            "torch.Size([128, 24201])\n",
            "torch.Size([128, 23059])\n",
            "torch.Size([128, 27420])\n",
            "torch.Size([128, 30539])\n",
            "torch.Size([128, 22832])\n",
            "torch.Size([128, 23737])\n",
            "torch.Size([128, 25334])\n",
            "torch.Size([128, 24979])\n",
            "torch.Size([128, 27501])\n",
            "torch.Size([128, 26386])\n",
            "torch.Size([128, 22397])\n",
            "torch.Size([128, 26058])\n",
            "torch.Size([128, 32049])\n",
            "torch.Size([128, 25187])\n",
            "torch.Size([128, 18764])\n",
            "torch.Size([128, 24071])\n",
            "torch.Size([128, 16996])\n",
            "torch.Size([128, 23455])\n",
            "torch.Size([128, 22511])\n",
            "torch.Size([128, 26063])\n",
            "torch.Size([128, 10267])\n",
            "torch.Size([128, 27877])\n",
            "torch.Size([128, 24852])\n",
            "torch.Size([128, 26513])\n",
            "torch.Size([128, 20377])\n",
            "torch.Size([128, 30493])\n",
            "torch.Size([128, 20965])\n",
            "torch.Size([128, 22134])\n",
            "torch.Size([128, 15465])\n",
            "torch.Size([128, 23097])\n",
            "torch.Size([128, 29624])\n",
            "torch.Size([128, 26439])\n",
            "torch.Size([128, 24308])\n",
            "torch.Size([128, 23138])\n",
            "torch.Size([128, 27277])\n",
            "torch.Size([128, 22663])\n",
            "torch.Size([128, 20385])\n",
            "torch.Size([128, 30169])\n",
            "torch.Size([128, 31424])\n",
            "torch.Size([128, 30260])\n",
            "torch.Size([128, 18368])\n",
            "torch.Size([128, 18389])\n",
            "torch.Size([128, 23489])\n",
            "torch.Size([128, 22912])\n",
            "torch.Size([128, 20815])\n",
            "torch.Size([128, 25864])\n",
            "torch.Size([128, 23200])\n",
            "torch.Size([128, 22927])\n",
            "torch.Size([128, 24312])\n",
            "torch.Size([128, 31580])\n",
            "torch.Size([128, 24090])\n",
            "torch.Size([128, 30129])\n",
            "torch.Size([128, 26338])\n",
            "torch.Size([128, 26119])\n",
            "torch.Size([128, 25559])\n",
            "torch.Size([128, 18649])\n",
            "torch.Size([128, 19895])\n",
            "torch.Size([128, 20981])\n",
            "torch.Size([128, 26779])\n",
            "torch.Size([128, 28449])\n",
            "torch.Size([128, 23454])\n",
            "torch.Size([128, 26027])\n",
            "torch.Size([128, 31305])\n",
            "torch.Size([128, 26317])\n",
            "torch.Size([128, 15294])\n",
            "torch.Size([128, 27976])\n",
            "torch.Size([128, 25845])\n",
            "torch.Size([128, 20396])\n",
            "torch.Size([128, 23882])\n",
            "torch.Size([128, 26577])\n",
            "torch.Size([128, 28241])\n",
            "torch.Size([128, 26760])\n",
            "torch.Size([128, 19432])\n",
            "torch.Size([128, 24452])\n",
            "torch.Size([128, 23440])\n",
            "torch.Size([128, 21709])\n",
            "torch.Size([128, 27638])\n",
            "torch.Size([128, 27130])\n",
            "torch.Size([128, 28233])\n",
            "torch.Size([128, 24973])\n",
            "torch.Size([128, 27139])\n",
            "torch.Size([128, 26429])\n",
            "torch.Size([128, 25486])\n",
            "torch.Size([128, 27708])\n",
            "torch.Size([128, 27757])\n",
            "torch.Size([128, 19699])\n",
            "torch.Size([128, 27331])\n",
            "torch.Size([128, 30695])\n",
            "torch.Size([128, 27432])\n",
            "torch.Size([128, 27657])\n",
            "torch.Size([128, 26158])\n",
            "torch.Size([128, 22488])\n",
            "torch.Size([128, 24288])\n",
            "torch.Size([128, 36920])\n",
            "torch.Size([128, 29552])\n",
            "torch.Size([128, 15747])\n",
            "torch.Size([128, 25405])\n",
            "torch.Size([128, 28158])\n",
            "torch.Size([128, 30891])\n",
            "torch.Size([128, 22270])\n",
            "torch.Size([128, 26370])\n",
            "torch.Size([128, 24773])\n",
            "torch.Size([128, 25655])\n",
            "torch.Size([128, 24189])\n",
            "torch.Size([128, 35453])\n",
            "torch.Size([128, 26970])\n",
            "torch.Size([128, 22092])\n",
            "torch.Size([128, 34147])\n",
            "torch.Size([128, 21679])\n",
            "torch.Size([128, 22716])\n",
            "torch.Size([128, 24784])\n",
            "torch.Size([128, 24294])\n",
            "torch.Size([128, 26560])\n",
            "torch.Size([128, 27081])\n",
            "torch.Size([128, 25371])\n",
            "torch.Size([128, 24781])\n",
            "torch.Size([128, 18982])\n",
            "torch.Size([128, 27086])\n",
            "torch.Size([128, 16572])\n",
            "torch.Size([128, 22276])\n",
            "torch.Size([128, 27497])\n",
            "torch.Size([128, 25764])\n",
            "torch.Size([128, 26274])\n",
            "torch.Size([128, 25852])\n",
            "torch.Size([128, 20058])\n",
            "torch.Size([128, 22064])\n",
            "torch.Size([128, 22990])\n",
            "torch.Size([128, 29616])\n",
            "torch.Size([128, 26070])\n",
            "torch.Size([128, 25860])\n",
            "torch.Size([128, 27513])\n",
            "torch.Size([128, 29099])\n",
            "torch.Size([128, 24252])\n",
            "torch.Size([128, 22212])\n",
            "torch.Size([128, 31722])\n",
            "torch.Size([128, 36442])\n",
            "torch.Size([128, 25245])\n",
            "torch.Size([128, 23073])\n",
            "torch.Size([128, 23620])\n",
            "torch.Size([128, 21655])\n",
            "torch.Size([128, 27425])\n",
            "torch.Size([128, 31227])\n",
            "torch.Size([128, 25926])\n",
            "torch.Size([128, 27332])\n",
            "torch.Size([128, 21253])\n",
            "torch.Size([128, 30119])\n",
            "torch.Size([128, 17484])\n",
            "torch.Size([128, 9843])\n",
            "torch.Size([128, 25433])\n",
            "torch.Size([128, 22455])\n",
            "torch.Size([128, 20126])\n",
            "torch.Size([128, 21032])\n",
            "torch.Size([128, 23991])\n",
            "torch.Size([128, 22447])\n",
            "torch.Size([128, 27376])\n",
            "torch.Size([128, 22130])\n",
            "torch.Size([128, 23550])\n",
            "torch.Size([128, 26083])\n",
            "torch.Size([128, 26852])\n",
            "torch.Size([128, 19309])\n",
            "torch.Size([128, 20462])\n",
            "torch.Size([128, 27927])\n",
            "torch.Size([128, 20777])\n",
            "torch.Size([128, 28645])\n",
            "torch.Size([128, 29451])\n",
            "torch.Size([128, 20893])\n",
            "torch.Size([128, 25258])\n",
            "torch.Size([128, 21429])\n",
            "torch.Size([128, 16907])\n",
            "torch.Size([128, 24964])\n",
            "torch.Size([128, 27807])\n",
            "torch.Size([128, 20784])\n",
            "torch.Size([128, 24599])\n",
            "torch.Size([128, 17042])\n",
            "torch.Size([128, 21051])\n",
            "torch.Size([128, 25983])\n",
            "torch.Size([128, 18184])\n",
            "torch.Size([128, 23048])\n",
            "torch.Size([128, 24684])\n",
            "torch.Size([128, 23679])\n",
            "torch.Size([128, 15499])\n",
            "torch.Size([128, 31233])\n",
            "torch.Size([128, 30844])\n",
            "torch.Size([128, 27214])\n",
            "torch.Size([128, 26888])\n",
            "torch.Size([128, 22866])\n",
            "torch.Size([128, 27626])\n",
            "torch.Size([128, 24181])\n",
            "torch.Size([128, 26192])\n",
            "torch.Size([128, 27338])\n",
            "torch.Size([128, 25452])\n",
            "torch.Size([128, 17914])\n",
            "torch.Size([128, 24799])\n",
            "torch.Size([128, 19128])\n",
            "torch.Size([128, 22599])\n",
            "torch.Size([128, 29035])\n",
            "torch.Size([128, 27693])\n",
            "torch.Size([128, 24965])\n",
            "torch.Size([128, 26873])\n",
            "torch.Size([128, 27539])\n",
            "torch.Size([128, 25930])\n",
            "torch.Size([128, 21513])\n",
            "torch.Size([128, 10991])\n",
            "torch.Size([128, 25163])\n",
            "torch.Size([128, 29325])\n",
            "torch.Size([128, 25365])\n",
            "torch.Size([128, 23132])\n",
            "torch.Size([128, 21697])\n",
            "torch.Size([128, 32287])\n",
            "torch.Size([128, 26558])\n",
            "torch.Size([128, 26897])\n",
            "torch.Size([128, 24782])\n",
            "torch.Size([128, 22363])\n",
            "torch.Size([128, 27705])\n",
            "torch.Size([128, 30195])\n",
            "torch.Size([128, 25870])\n",
            "torch.Size([128, 29314])\n",
            "torch.Size([128, 27007])\n",
            "torch.Size([128, 23392])\n",
            "torch.Size([128, 20736])\n",
            "torch.Size([128, 25394])\n",
            "torch.Size([128, 28243])\n",
            "torch.Size([128, 22402])\n",
            "torch.Size([128, 15519])\n",
            "torch.Size([128, 20368])\n",
            "torch.Size([128, 21250])\n",
            "torch.Size([128, 16403])\n",
            "torch.Size([128, 29933])\n",
            "torch.Size([128, 15536])\n",
            "torch.Size([128, 24272])\n",
            "torch.Size([128, 28243])\n",
            "torch.Size([128, 21271])\n",
            "torch.Size([128, 29312])\n",
            "torch.Size([128, 23771])\n",
            "torch.Size([128, 31063])\n",
            "torch.Size([128, 20074])\n",
            "torch.Size([128, 21849])\n",
            "torch.Size([128, 26924])\n",
            "torch.Size([128, 25347])\n",
            "torch.Size([128, 22266])\n",
            "torch.Size([128, 31022])\n",
            "torch.Size([128, 29583])\n",
            "torch.Size([128, 27498])\n",
            "torch.Size([128, 24901])\n",
            "torch.Size([128, 23789])\n",
            "torch.Size([128, 31827])\n",
            "torch.Size([128, 26730])\n",
            "torch.Size([128, 29254])\n",
            "torch.Size([128, 18740])\n",
            "torch.Size([128, 21687])\n",
            "torch.Size([128, 26820])\n",
            "torch.Size([128, 22731])\n",
            "torch.Size([128, 24336])\n",
            "torch.Size([128, 21469])\n",
            "torch.Size([128, 36552])\n",
            "torch.Size([128, 25986])\n",
            "torch.Size([128, 27793])\n",
            "torch.Size([128, 23106])\n",
            "torch.Size([128, 24229])\n",
            "torch.Size([128, 24273])\n",
            "torch.Size([128, 27683])\n",
            "torch.Size([128, 19431])\n",
            "torch.Size([128, 30320])\n",
            "torch.Size([128, 20384])\n",
            "torch.Size([128, 27797])\n",
            "torch.Size([128, 27442])\n",
            "torch.Size([128, 21328])\n",
            "torch.Size([128, 22451])\n",
            "torch.Size([128, 22368])\n",
            "torch.Size([128, 20513])\n",
            "torch.Size([128, 26189])\n",
            "torch.Size([128, 29029])\n",
            "torch.Size([128, 23065])\n",
            "torch.Size([128, 25882])\n",
            "torch.Size([128, 24221])\n",
            "torch.Size([128, 24301])\n",
            "torch.Size([128, 25727])\n",
            "torch.Size([128, 24852])\n",
            "torch.Size([128, 29402])\n",
            "torch.Size([128, 19858])\n",
            "torch.Size([128, 23950])\n",
            "torch.Size([128, 29495])\n",
            "torch.Size([128, 30845])\n",
            "torch.Size([128, 20577])\n",
            "torch.Size([128, 23803])\n",
            "torch.Size([128, 27308])\n",
            "torch.Size([128, 23877])\n",
            "torch.Size([128, 14530])\n",
            "torch.Size([128, 30882])\n",
            "torch.Size([128, 27311])\n",
            "torch.Size([128, 22896])\n",
            "torch.Size([128, 23618])\n",
            "torch.Size([128, 24936])\n",
            "torch.Size([128, 25931])\n",
            "torch.Size([128, 25949])\n",
            "torch.Size([128, 27305])\n",
            "torch.Size([128, 23306])\n",
            "torch.Size([128, 26839])\n",
            "torch.Size([128, 23704])\n",
            "torch.Size([128, 25508])\n",
            "torch.Size([128, 28042])\n",
            "torch.Size([128, 23431])\n",
            "torch.Size([128, 17054])\n",
            "torch.Size([128, 27160])\n",
            "torch.Size([128, 22545])\n",
            "torch.Size([128, 22436])\n",
            "torch.Size([128, 26532])\n",
            "torch.Size([128, 27908])\n",
            "torch.Size([128, 28989])\n",
            "torch.Size([128, 23871])\n",
            "torch.Size([128, 18060])\n",
            "torch.Size([128, 27964])\n",
            "torch.Size([128, 23152])\n",
            "torch.Size([128, 24071])\n",
            "torch.Size([128, 25069])\n",
            "torch.Size([128, 28606])\n",
            "torch.Size([128, 18661])\n",
            "torch.Size([128, 25694])\n",
            "torch.Size([128, 27223])\n",
            "torch.Size([128, 20931])\n",
            "torch.Size([128, 26819])\n",
            "torch.Size([128, 23955])\n",
            "torch.Size([128, 19397])\n",
            "torch.Size([128, 23912])\n",
            "torch.Size([128, 28036])\n",
            "torch.Size([128, 33709])\n",
            "torch.Size([128, 25614])\n",
            "torch.Size([128, 22307])\n",
            "torch.Size([128, 23976])\n",
            "torch.Size([128, 28752])\n",
            "torch.Size([128, 23864])\n",
            "torch.Size([128, 26148])\n",
            "torch.Size([128, 24202])\n",
            "torch.Size([128, 27577])\n",
            "torch.Size([128, 23429])\n",
            "torch.Size([128, 26997])\n",
            "torch.Size([128, 29998])\n",
            "torch.Size([128, 18842])\n",
            "torch.Size([128, 19296])\n",
            "torch.Size([128, 24973])\n",
            "torch.Size([128, 24328])\n",
            "torch.Size([128, 19881])\n",
            "torch.Size([128, 22390])\n",
            "torch.Size([128, 26682])\n",
            "torch.Size([128, 21447])\n",
            "torch.Size([128, 23677])\n",
            "torch.Size([128, 25576])\n",
            "torch.Size([128, 25522])\n",
            "torch.Size([128, 25382])\n",
            "torch.Size([128, 24215])\n",
            "torch.Size([128, 27106])\n",
            "torch.Size([128, 25092])\n",
            "torch.Size([128, 24654])\n",
            "torch.Size([128, 25388])\n",
            "torch.Size([128, 22704])\n",
            "torch.Size([128, 22603])\n",
            "torch.Size([128, 32557])\n",
            "torch.Size([128, 26207])\n",
            "torch.Size([128, 24238])\n",
            "torch.Size([128, 31102])\n",
            "torch.Size([128, 27931])\n",
            "torch.Size([128, 28406])\n",
            "torch.Size([128, 24715])\n",
            "torch.Size([128, 26169])\n",
            "torch.Size([128, 29158])\n",
            "torch.Size([128, 20884])\n",
            "torch.Size([128, 27270])\n",
            "torch.Size([128, 27923])\n",
            "torch.Size([128, 26814])\n",
            "torch.Size([128, 24224])\n",
            "torch.Size([128, 26723])\n",
            "torch.Size([128, 23423])\n",
            "torch.Size([128, 22316])\n",
            "torch.Size([128, 19630])\n",
            "torch.Size([128, 24908])\n",
            "torch.Size([128, 24736])\n",
            "torch.Size([128, 20307])\n",
            "torch.Size([128, 21520])\n",
            "torch.Size([128, 21154])\n",
            "torch.Size([128, 24935])\n",
            "torch.Size([128, 29807])\n",
            "torch.Size([128, 19394])\n",
            "torch.Size([128, 19168])\n",
            "torch.Size([128, 22338])\n",
            "torch.Size([128, 27708])\n",
            "torch.Size([128, 18166])\n",
            "torch.Size([128, 28427])\n",
            "torch.Size([128, 26386])\n",
            "torch.Size([128, 22380])\n",
            "torch.Size([128, 20341])\n",
            "torch.Size([128, 27158])\n",
            "torch.Size([128, 23441])\n",
            "torch.Size([128, 27908])\n",
            "torch.Size([128, 27275])\n",
            "torch.Size([128, 27610])\n",
            "torch.Size([128, 29319])\n",
            "torch.Size([128, 28827])\n",
            "torch.Size([128, 24125])\n",
            "torch.Size([128, 20745])\n",
            "torch.Size([128, 23820])\n",
            "torch.Size([128, 28666])\n",
            "torch.Size([128, 31997])\n",
            "torch.Size([128, 22010])\n",
            "torch.Size([128, 19796])\n",
            "torch.Size([128, 27889])\n",
            "torch.Size([128, 22847])\n",
            "torch.Size([128, 32158])\n",
            "torch.Size([128, 17619])\n",
            "torch.Size([128, 28957])\n",
            "torch.Size([128, 27026])\n",
            "torch.Size([128, 24162])\n",
            "torch.Size([128, 25360])\n",
            "torch.Size([128, 27666])\n",
            "torch.Size([128, 24782])\n",
            "torch.Size([128, 27694])\n",
            "torch.Size([128, 25437])\n",
            "torch.Size([128, 29602])\n",
            "torch.Size([128, 18591])\n",
            "torch.Size([128, 28119])\n",
            "torch.Size([128, 32188])\n",
            "torch.Size([128, 23845])\n",
            "torch.Size([128, 20585])\n",
            "torch.Size([128, 21911])\n",
            "torch.Size([128, 20549])\n",
            "torch.Size([128, 26579])\n",
            "torch.Size([128, 22692])\n",
            "torch.Size([128, 20366])\n",
            "torch.Size([128, 26241])\n",
            "torch.Size([128, 24521])\n",
            "torch.Size([128, 23051])\n",
            "torch.Size([128, 25682])\n",
            "torch.Size([128, 15347])\n",
            "torch.Size([128, 24930])\n",
            "torch.Size([128, 26570])\n",
            "torch.Size([128, 29575])\n",
            "torch.Size([128, 24731])\n",
            "torch.Size([128, 27601])\n",
            "torch.Size([128, 23055])\n",
            "torch.Size([128, 26470])\n",
            "torch.Size([128, 36777])\n",
            "torch.Size([128, 22822])\n",
            "torch.Size([128, 23533])\n",
            "torch.Size([128, 27976])\n",
            "torch.Size([128, 24161])\n",
            "torch.Size([128, 27542])\n",
            "torch.Size([128, 25607])\n",
            "torch.Size([128, 26127])\n",
            "torch.Size([128, 19647])\n",
            "torch.Size([128, 23830])\n",
            "torch.Size([128, 18873])\n",
            "torch.Size([128, 22758])\n",
            "torch.Size([128, 30025])\n",
            "torch.Size([128, 12317])\n",
            "torch.Size([128, 25814])\n",
            "torch.Size([128, 20578])\n",
            "torch.Size([128, 26902])\n",
            "torch.Size([128, 22207])\n",
            "torch.Size([128, 20284])\n",
            "torch.Size([128, 26539])\n",
            "torch.Size([128, 29043])\n",
            "torch.Size([128, 32633])\n",
            "torch.Size([128, 19082])\n",
            "torch.Size([128, 27649])\n",
            "torch.Size([128, 29575])\n",
            "torch.Size([128, 28158])\n",
            "torch.Size([128, 20050])\n",
            "torch.Size([128, 30145])\n",
            "torch.Size([128, 24485])\n",
            "torch.Size([128, 29331])\n",
            "torch.Size([128, 30760])\n",
            "torch.Size([128, 24216])\n",
            "torch.Size([128, 26924])\n",
            "torch.Size([128, 25147])\n",
            "torch.Size([128, 26787])\n",
            "torch.Size([128, 32008])\n",
            "torch.Size([128, 31952])\n",
            "torch.Size([128, 25474])\n",
            "torch.Size([128, 23899])\n",
            "torch.Size([128, 25905])\n",
            "torch.Size([128, 24207])\n",
            "torch.Size([128, 30151])\n",
            "torch.Size([128, 25533])\n",
            "torch.Size([128, 4466])\n",
            "torch.Size([128, 25500])\n",
            "torch.Size([128, 25441])\n",
            "torch.Size([128, 26254])\n",
            "torch.Size([128, 32302])\n",
            "torch.Size([128, 27483])\n",
            "torch.Size([128, 28574])\n",
            "torch.Size([128, 19990])\n",
            "torch.Size([128, 22983])\n",
            "torch.Size([128, 20662])\n",
            "torch.Size([128, 23649])\n",
            "torch.Size([128, 25119])\n",
            "torch.Size([128, 25730])\n",
            "torch.Size([128, 26760])\n",
            "torch.Size([128, 31313])\n",
            "torch.Size([128, 28355])\n",
            "torch.Size([128, 25094])\n",
            "torch.Size([128, 21721])\n",
            "torch.Size([128, 17619])\n",
            "torch.Size([128, 34113])\n",
            "torch.Size([128, 23806])\n",
            "torch.Size([128, 28729])\n",
            "torch.Size([128, 26132])\n",
            "torch.Size([128, 23666])\n",
            "torch.Size([128, 20612])\n",
            "torch.Size([128, 28305])\n",
            "torch.Size([128, 28820])\n",
            "torch.Size([128, 21362])\n",
            "torch.Size([128, 27677])\n",
            "torch.Size([128, 15619])\n",
            "torch.Size([128, 21940])\n",
            "torch.Size([128, 30233])\n",
            "torch.Size([128, 21968])\n",
            "torch.Size([128, 22651])\n",
            "torch.Size([128, 24441])\n",
            "torch.Size([128, 24804])\n",
            "torch.Size([128, 28133])\n",
            "torch.Size([128, 27373])\n",
            "torch.Size([128, 25839])\n",
            "torch.Size([128, 33956])\n",
            "torch.Size([128, 23615])\n",
            "torch.Size([128, 25726])\n",
            "torch.Size([128, 29961])\n",
            "torch.Size([128, 25127])\n",
            "torch.Size([128, 29893])\n",
            "torch.Size([128, 18667])\n",
            "torch.Size([128, 24917])\n",
            "torch.Size([128, 24059])\n",
            "torch.Size([128, 19310])\n",
            "torch.Size([128, 26044])\n",
            "torch.Size([128, 25360])\n",
            "torch.Size([128, 29620])\n",
            "torch.Size([128, 29618])\n",
            "torch.Size([128, 25119])\n",
            "torch.Size([128, 23445])\n",
            "torch.Size([128, 26689])\n",
            "torch.Size([128, 23980])\n",
            "torch.Size([128, 23812])\n",
            "torch.Size([128, 25307])\n",
            "torch.Size([128, 25476])\n",
            "torch.Size([128, 26464])\n",
            "torch.Size([128, 25658])\n",
            "torch.Size([128, 22335])\n",
            "torch.Size([128, 28175])\n",
            "torch.Size([128, 24704])\n",
            "torch.Size([128, 20965])\n",
            "torch.Size([128, 27581])\n",
            "torch.Size([128, 20581])\n",
            "torch.Size([128, 24178])\n",
            "torch.Size([128, 29255])\n",
            "torch.Size([128, 30502])\n",
            "torch.Size([128, 30856])\n",
            "torch.Size([128, 25948])\n",
            "torch.Size([128, 27205])\n",
            "torch.Size([128, 27127])\n",
            "torch.Size([128, 23611])\n",
            "torch.Size([128, 24431])\n",
            "torch.Size([128, 27120])\n",
            "torch.Size([128, 27273])\n",
            "torch.Size([128, 14152])\n",
            "torch.Size([128, 24570])\n",
            "torch.Size([128, 26945])\n",
            "torch.Size([128, 27070])\n",
            "torch.Size([128, 21131])\n",
            "torch.Size([128, 20159])\n",
            "torch.Size([128, 27783])\n",
            "torch.Size([128, 23646])\n",
            "torch.Size([128, 19833])\n",
            "torch.Size([128, 21276])\n",
            "torch.Size([128, 21789])\n",
            "torch.Size([128, 24567])\n",
            "torch.Size([128, 19485])\n",
            "torch.Size([128, 26963])\n",
            "torch.Size([128, 25852])\n",
            "torch.Size([128, 19853])\n",
            "torch.Size([128, 18861])\n",
            "torch.Size([128, 23514])\n",
            "torch.Size([128, 8817])\n",
            "torch.Size([128, 31607])\n",
            "torch.Size([128, 25526])\n",
            "torch.Size([128, 20208])\n",
            "torch.Size([128, 21960])\n",
            "torch.Size([128, 18947])\n",
            "torch.Size([128, 27989])\n",
            "torch.Size([128, 26261])\n",
            "torch.Size([128, 20158])\n",
            "torch.Size([128, 22951])\n",
            "torch.Size([128, 23167])\n",
            "torch.Size([128, 21623])\n",
            "torch.Size([128, 26242])\n",
            "torch.Size([128, 29929])\n",
            "torch.Size([128, 31167])\n",
            "torch.Size([128, 21825])\n",
            "torch.Size([128, 26297])\n",
            "torch.Size([128, 31780])\n",
            "torch.Size([128, 25114])\n",
            "torch.Size([128, 16638])\n",
            "torch.Size([128, 24753])\n",
            "torch.Size([128, 36705])\n",
            "torch.Size([128, 21111])\n",
            "torch.Size([128, 31654])\n",
            "torch.Size([128, 19089])\n",
            "torch.Size([128, 25285])\n",
            "torch.Size([128, 29397])\n",
            "torch.Size([128, 26355])\n",
            "torch.Size([128, 28931])\n",
            "torch.Size([128, 20524])\n",
            "torch.Size([128, 23591])\n",
            "torch.Size([128, 24210])\n",
            "torch.Size([128, 16359])\n",
            "torch.Size([128, 9484])\n",
            "torch.Size([128, 16948])\n",
            "torch.Size([128, 30888])\n",
            "torch.Size([128, 31886])\n",
            "torch.Size([128, 25187])\n",
            "torch.Size([128, 25409])\n",
            "torch.Size([128, 27041])\n",
            "torch.Size([128, 26543])\n",
            "torch.Size([128, 29775])\n",
            "torch.Size([128, 26306])\n",
            "torch.Size([128, 23800])\n",
            "torch.Size([128, 24760])\n",
            "torch.Size([128, 29705])\n",
            "torch.Size([128, 26762])\n",
            "torch.Size([128, 25932])\n",
            "torch.Size([128, 22574])\n",
            "torch.Size([128, 17047])\n",
            "torch.Size([128, 23816])\n",
            "torch.Size([128, 30345])\n",
            "torch.Size([128, 23326])\n",
            "torch.Size([128, 24321])\n",
            "torch.Size([128, 25098])\n",
            "torch.Size([128, 27224])\n",
            "torch.Size([128, 28954])\n",
            "torch.Size([128, 11694])\n",
            "torch.Size([128, 26880])\n",
            "torch.Size([128, 28027])\n",
            "torch.Size([128, 27280])\n",
            "torch.Size([128, 27924])\n",
            "torch.Size([128, 29222])\n",
            "torch.Size([128, 25544])\n",
            "torch.Size([128, 22962])\n",
            "torch.Size([128, 22321])\n",
            "torch.Size([128, 26549])\n",
            "torch.Size([128, 25658])\n",
            "torch.Size([128, 23714])\n",
            "torch.Size([128, 25844])\n",
            "torch.Size([128, 25020])\n",
            "torch.Size([128, 22134])\n",
            "torch.Size([128, 23123])\n",
            "torch.Size([128, 25908])\n",
            "torch.Size([128, 26772])\n",
            "torch.Size([128, 14084])\n",
            "torch.Size([128, 25952])\n",
            "torch.Size([128, 23932])\n",
            "torch.Size([128, 17559])\n",
            "torch.Size([128, 28060])\n",
            "torch.Size([128, 28120])\n",
            "torch.Size([128, 21494])\n",
            "torch.Size([128, 24989])\n",
            "torch.Size([128, 26829])\n",
            "torch.Size([128, 22458])\n",
            "torch.Size([128, 25987])\n",
            "torch.Size([128, 19771])\n",
            "torch.Size([128, 24780])\n",
            "torch.Size([128, 30835])\n",
            "torch.Size([128, 26633])\n",
            "torch.Size([128, 25707])\n",
            "torch.Size([128, 27498])\n",
            "torch.Size([128, 22283])\n",
            "torch.Size([128, 26244])\n",
            "torch.Size([128, 20027])\n",
            "torch.Size([128, 47115])\n",
            "torch.Size([128, 22883])\n",
            "torch.Size([128, 26695])\n",
            "torch.Size([128, 25792])\n",
            "torch.Size([128, 27992])\n",
            "torch.Size([128, 26539])\n",
            "torch.Size([128, 29058])\n",
            "torch.Size([128, 18983])\n",
            "torch.Size([128, 22502])\n",
            "torch.Size([128, 28633])\n",
            "torch.Size([128, 19698])\n",
            "torch.Size([128, 27086])\n",
            "torch.Size([128, 27513])\n",
            "torch.Size([128, 26817])\n",
            "torch.Size([128, 26074])\n",
            "torch.Size([128, 25777])\n",
            "torch.Size([128, 26429])\n",
            "torch.Size([128, 25204])\n",
            "torch.Size([128, 29148])\n",
            "torch.Size([128, 22711])\n",
            "torch.Size([128, 18266])\n",
            "torch.Size([128, 19077])\n",
            "torch.Size([128, 26479])\n",
            "torch.Size([128, 25923])\n",
            "torch.Size([128, 22691])\n",
            "torch.Size([128, 25374])\n",
            "torch.Size([128, 27870])\n",
            "torch.Size([128, 26756])\n",
            "torch.Size([128, 28400])\n",
            "torch.Size([128, 24464])\n",
            "torch.Size([128, 20493])\n",
            "torch.Size([128, 27113])\n",
            "torch.Size([128, 30410])\n",
            "torch.Size([128, 22356])\n",
            "torch.Size([128, 24299])\n",
            "torch.Size([128, 31502])\n",
            "torch.Size([128, 31577])\n",
            "torch.Size([128, 20689])\n",
            "torch.Size([128, 31294])\n",
            "torch.Size([128, 27860])\n",
            "torch.Size([128, 20374])\n",
            "torch.Size([128, 20905])\n",
            "torch.Size([128, 17112])\n",
            "torch.Size([128, 31649])\n",
            "torch.Size([128, 25397])\n",
            "torch.Size([128, 22919])\n",
            "torch.Size([128, 25040])\n",
            "torch.Size([128, 26545])\n",
            "torch.Size([128, 27304])\n",
            "torch.Size([128, 30533])\n",
            "torch.Size([128, 18804])\n",
            "torch.Size([128, 28479])\n",
            "torch.Size([128, 20003])\n",
            "torch.Size([128, 28095])\n",
            "torch.Size([128, 28074])\n",
            "torch.Size([128, 24049])\n",
            "torch.Size([128, 24652])\n",
            "torch.Size([128, 24426])\n",
            "torch.Size([128, 22808])\n",
            "torch.Size([128, 29168])\n",
            "torch.Size([128, 26122])\n",
            "torch.Size([128, 25706])\n",
            "torch.Size([128, 19087])\n",
            "torch.Size([128, 7598])\n",
            "torch.Size([128, 23953])\n",
            "torch.Size([128, 32355])\n",
            "torch.Size([128, 28176])\n",
            "torch.Size([128, 27333])\n",
            "torch.Size([128, 27919])\n",
            "torch.Size([128, 22915])\n",
            "torch.Size([128, 18061])\n",
            "torch.Size([128, 26702])\n",
            "torch.Size([128, 12412])\n",
            "torch.Size([128, 25522])\n",
            "torch.Size([128, 20235])\n",
            "torch.Size([128, 18139])\n",
            "torch.Size([128, 24642])\n",
            "torch.Size([128, 27327])\n",
            "torch.Size([128, 24384])\n",
            "torch.Size([128, 29057])\n",
            "torch.Size([128, 22506])\n",
            "torch.Size([128, 23888])\n",
            "torch.Size([128, 29639])\n",
            "torch.Size([128, 26729])\n",
            "torch.Size([128, 26529])\n",
            "torch.Size([128, 28926])\n",
            "torch.Size([128, 22946])\n",
            "torch.Size([128, 20657])\n",
            "torch.Size([128, 21235])\n",
            "torch.Size([128, 25891])\n",
            "torch.Size([128, 22486])\n",
            "torch.Size([128, 19549])\n",
            "torch.Size([128, 25361])\n",
            "torch.Size([128, 30801])\n",
            "torch.Size([128, 22542])\n",
            "torch.Size([128, 30145])\n",
            "torch.Size([128, 23604])\n",
            "torch.Size([128, 24480])\n",
            "torch.Size([128, 20691])\n",
            "torch.Size([128, 26276])\n",
            "torch.Size([128, 26979])\n",
            "torch.Size([128, 19560])\n",
            "torch.Size([128, 7256])\n",
            "torch.Size([128, 22311])\n",
            "torch.Size([128, 25883])\n",
            "torch.Size([128, 22590])\n",
            "torch.Size([128, 18628])\n",
            "torch.Size([128, 24030])\n",
            "torch.Size([128, 30089])\n",
            "torch.Size([128, 26518])\n",
            "torch.Size([128, 18926])\n",
            "torch.Size([128, 33844])\n",
            "torch.Size([128, 23793])\n",
            "torch.Size([128, 24467])\n",
            "torch.Size([128, 29435])\n",
            "torch.Size([128, 28961])\n",
            "torch.Size([128, 24631])\n",
            "torch.Size([128, 25305])\n",
            "torch.Size([128, 26008])\n",
            "torch.Size([128, 21928])\n",
            "torch.Size([128, 30504])\n",
            "torch.Size([128, 27577])\n",
            "torch.Size([128, 24487])\n",
            "torch.Size([128, 26649])\n",
            "torch.Size([128, 27151])\n",
            "torch.Size([128, 24894])\n",
            "torch.Size([128, 23386])\n",
            "torch.Size([128, 28170])\n",
            "torch.Size([128, 23932])\n",
            "torch.Size([128, 24354])\n",
            "torch.Size([128, 22886])\n",
            "torch.Size([128, 22527])\n",
            "torch.Size([128, 27723])\n",
            "torch.Size([128, 22578])\n",
            "torch.Size([128, 27435])\n",
            "torch.Size([128, 29545])\n",
            "torch.Size([128, 24338])\n",
            "torch.Size([128, 20879])\n",
            "torch.Size([128, 28830])\n",
            "torch.Size([128, 24231])\n",
            "torch.Size([128, 21449])\n",
            "torch.Size([128, 29306])\n",
            "torch.Size([128, 23946])\n",
            "torch.Size([128, 25240])\n",
            "torch.Size([128, 26058])\n",
            "torch.Size([128, 21429])\n",
            "torch.Size([128, 28114])\n",
            "torch.Size([128, 23617])\n",
            "torch.Size([128, 17504])\n",
            "torch.Size([128, 21950])\n",
            "torch.Size([128, 36575])\n",
            "torch.Size([128, 21897])\n",
            "torch.Size([128, 22874])\n",
            "torch.Size([128, 34360])\n",
            "torch.Size([128, 25544])\n",
            "torch.Size([128, 23698])\n",
            "torch.Size([128, 21123])\n",
            "torch.Size([128, 26069])\n",
            "torch.Size([128, 27883])\n",
            "torch.Size([128, 21935])\n",
            "torch.Size([128, 28867])\n",
            "torch.Size([128, 26783])\n",
            "torch.Size([128, 23785])\n",
            "torch.Size([128, 27054])\n",
            "torch.Size([128, 25354])\n",
            "torch.Size([128, 17059])\n",
            "torch.Size([128, 24836])\n",
            "torch.Size([128, 15901])\n",
            "torch.Size([128, 29787])\n",
            "torch.Size([128, 24415])\n",
            "torch.Size([128, 23861])\n",
            "torch.Size([128, 28788])\n",
            "torch.Size([128, 11880])\n",
            "torch.Size([128, 24097])\n",
            "torch.Size([128, 24814])\n",
            "torch.Size([128, 20310])\n",
            "torch.Size([128, 28466])\n",
            "torch.Size([128, 29033])\n",
            "torch.Size([128, 17527])\n",
            "torch.Size([128, 24036])\n",
            "torch.Size([128, 19750])\n",
            "torch.Size([128, 23643])\n",
            "torch.Size([128, 28901])\n",
            "torch.Size([128, 25571])\n",
            "torch.Size([128, 30310])\n",
            "torch.Size([128, 22825])\n",
            "torch.Size([128, 24998])\n",
            "torch.Size([128, 28282])\n",
            "torch.Size([128, 20172])\n",
            "torch.Size([128, 18853])\n",
            "torch.Size([128, 30307])\n",
            "torch.Size([128, 24704])\n",
            "torch.Size([128, 31261])\n",
            "torch.Size([128, 22575])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vTxlEUDlC_5y",
        "outputId": "48c390e7-ac0b-4eb7-c594-cc36c6c5e542"
      },
      "source": [
        "import IPython.display\n",
        "\n",
        "# cello_c_chord.write('cello-C-chord.mid')\n",
        "pm = piano_roll_to_pretty_midi(train_dataset[2])\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b65918494967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# cello_c_chord.write('cello-C-chord.mid')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiano_roll_to_pretty_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'piano_roll_to_pretty_midi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pdqrt0uFqn6"
      },
      "source": [
        "#were in bussiness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "IBO96-EtFuY4"
      },
      "source": [
        "#@title KWTA\n",
        "\n",
        "\n",
        "class SparsifyBase(nn.Module):\n",
        "    def __init__(self, sparse_ratio=0.5):\n",
        "        super(SparsifyBase, self).__init__()\n",
        "        self.sr = sparse_ratio\n",
        "        self.preact = None\n",
        "        self.act = None\n",
        "    def get_activation(self):\n",
        "        def hook(model, input, output):\n",
        "            self.preact = input[0].cpu().detach().clone()\n",
        "            self.act = output.cpu().detach().clone()\n",
        "        return hook\n",
        "    def record_activation(self):\n",
        "        self.register_forward_hook(self.get_activation())\n",
        "\n",
        "\n",
        "class Sparsify1D_kactive(SparsifyBase):\n",
        "    def __init__(self, k=1):\n",
        "        super(Sparsify1D_kactive, self).__init__()\n",
        "        self.k = k\n",
        "    def forward(self, x):\n",
        "        m = torch.zeros(x.shape).to(device)\n",
        "        for i in range(self.k):\n",
        "            indeces = x.topk(self.k, dim=1)[1][:, i]\n",
        "            m += torch.mul(torch.zeros(x.shape).to(device).scatter(1, indeces.unsqueeze(1), 1), x)\n",
        "            # print(\"\\n hi\", m )\n",
        "        return m.double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "xgpIeeomFwls"
      },
      "source": [
        "#@title TCN \n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 1)\n",
        "        self.conv2.weight.data.normal_(0, 1)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"block \", x.size())\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "        # print(\"last layer conv\", self.network[-1].conv2.weight.data[:,0,:].size())\n",
        "        # print(\"last layer conv\", self.network[-1].conv2.weight.data[:,0,:])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Onb8eMTOFzUz"
      },
      "source": [
        "#@title TCN - Autoeconder \n",
        "\n",
        "class TCNAutoencoder(nn.Module):\n",
        "    def __init__(self, kernel_size, dropout, wta_k):\n",
        "        super(TCNAutoencoder, self).__init__()\n",
        "        self.wta = Sparsify1D_kactive(k = wta_k)\n",
        "        self.feature = TemporalConvNet(128, [128*2,128*4], kernel_size, dropout=dropout).double()\n",
        "        self.encoder = torch.nn.Conv1d(in_channels=128*4, out_channels=200, kernel_size=kernel_size, padding=0, bias=True, stride=4)\n",
        "        self.decoder = torch.nn.ConvTranspose1d(in_channels=200, out_channels=1, kernel_size=kernel_size, padding=0, bias=True, stride=4)\n",
        "        # self.encoder.weight.data.normal_(30)\n",
        "        # self.decoder.weight.data.normal_(300)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.code = None\n",
        "        # torch.nn.init.xavier_uniform(self.encoder.weight)\n",
        "        # torch.nn.init.xavier_uniform(self.decoder.weight)\n",
        "    def get_kernels(self):\n",
        "        return self.decoder.weight.data[:,0,:]\n",
        "    def feature_map(self, x):\n",
        "        code = self.code\n",
        "        return code\n",
        "    def forward(self, x):\n",
        "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
        "        output = self.feature(x)\n",
        "        self.code = self.wta(self.encoder(output))\n",
        "        output = self.decoder(self.code )\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "yrrRt_sTF1TL"
      },
      "source": [
        "#@title GO\n",
        "train_dataset = data \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "model = TCNAutoencoder(kernel_size=4, \n",
        "                       dropout=0.2, \n",
        "                       wta_k = 95).to(device).double()\n",
        "print(\"TCNAutoencoder trainable parameters: \", get_model_parameters(model))\n",
        "\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.MSELoss().to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=.01, weight_decay = 0.00001, momentum=0.05) ##this has weight decay just like you implemented\n",
        "optimizer = optim.AdamW(model.parameters(), lr=.005,  betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True) ##this has weight decay just like you implemented\n",
        "epochs = 60\n",
        "history = {\"loss\": []}\n",
        "\n",
        "calc = []\n",
        "total_len = 0\n",
        "for i in range(epochs):\n",
        "    #decaying WTA\n",
        "    if i % 12 == 0 and i != 0:\n",
        "        model.wta.k = max(1, model.wta.k - 5)\n",
        "        print(\"model.wta.k: \", model.wta.k)\n",
        "    for train_data in train_dataset:\n",
        "        # calc.extend(train_data.flatten().numpy())\n",
        "        #normalize \n",
        "        # train_data = (train_data - 224.15541543187527) / 111.14747885919755\n",
        "        #preprocess\n",
        "        lenby4 = len(train_data) // 4\n",
        "        train_data = train_data[None, None, 0:lenby4*4].to(device).double()\n",
        "        \n",
        "        #preprocess\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_data)\n",
        "\n",
        "        loss = loss_fn(output, train_data)\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        history[\"loss\"].append(float(loss)*lenby4)\n",
        "        total_len += lenby4\n",
        "    print(\"Epoch : {} \\t Loss : {} \".format(i, round(float(np.mean(history[\"loss\"], axis=0)/total_len),7)))\n",
        "    history[\"loss\"] = []\n",
        "    total_len = 0\n",
        "\n",
        "# print(len(calc))\n",
        "# print(np.mean(calc, axis=0))\n",
        "# print(np.std(calc, axis=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}