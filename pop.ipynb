{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOj7Kqblugk/JnWAHnwXzct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/shift_invariant_dictionary_learning/blob/main/pop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "90844aba-5f8c-4820-8286-f08f12883da5"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "%cd /content/POP909-Dataset/data_process\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=324aaca91e8a3ff4820ca6aba0394fc6b3491e9c72146fae53e23e56e750d0e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 26.99 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/POP909-Dataset/data_process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3uwo-E8NFshn"
      },
      "source": [
        "#@title Pytorch for DL\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "\n",
        "def get_model_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePLQyAsOyc60",
        "outputId": "ca957d6c-832a-4925-aa75-90f8efa6038e"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "from processor import MidiEventProcessor\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "total = 0\n",
        "def preprocess_midi(path):\n",
        "    global total\n",
        "    data = pretty_midi.PrettyMIDI(path)\n",
        "    main_notes = []\n",
        "    acc_notes = []\n",
        "    for ins in data.instruments:\n",
        "        acc_notes.extend(ins.notes)\n",
        "    for i in range(len(main_notes)):\n",
        "        main_notes[i].start = round(main_notes[i].start,2)\n",
        "        main_notes[i].end = round(main_notes[i].end,2)\n",
        "    for i in range(len(acc_notes)):\n",
        "        acc_notes[i].start = round(acc_notes[i].start,2)\n",
        "        acc_notes[i].end = round(acc_notes[i].end,2)\n",
        "    main_notes.sort(key = lambda x:x.start)\n",
        "    acc_notes.sort(key = lambda x:x.start)\n",
        "\n",
        "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
        "    piano = pretty_midi.Instrument(program=piano_program)\n",
        "    piano.notes.extend(acc_notes)\n",
        "    # score = prettyn\n",
        "    # mpr = MidiEventProcessor()\n",
        "    # repr_seq = mpr.encode([main_notes, acc_notes])\n",
        "    total += 1\n",
        "    return piano.get_piano_roll()\n",
        "\n",
        "def preprocess_pop909(midi_root, save_dir):\n",
        "    save_py = []\n",
        "    midi_paths = [d for d in os.listdir(midi_root)] #not index\n",
        "    i = 0\n",
        "    out_fmt = '{}-{}.data'\n",
        "    for path in midi_paths:\n",
        "        if \".\" not in path:\n",
        "          # print(' ', end='[{}]'.format(path), flush=True)\n",
        "          filename = midi_root + path  +\"/\"+path[0:3] + \".mid\"\n",
        "          try:\n",
        "              data = torch.tensor(preprocess_midi(filename))\n",
        "              print(data.shape)\n",
        "          except KeyboardInterrupt:\n",
        "              print(' Abort')\n",
        "              return\n",
        "          except EOFError:\n",
        "              print('EOF Error')\n",
        "              return\n",
        "          save_py.append(data)\n",
        "    return save_py\n",
        "     \n",
        "    \n",
        "# replace the folder with your POP909 data folder\n",
        "train_dataset = preprocess_pop909(\"../POP909/\",\"midi_data/\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 26426])\n",
            "torch.Size([128, 16903])\n",
            "torch.Size([128, 21515])\n",
            "torch.Size([128, 24289])\n",
            "torch.Size([128, 27367])\n",
            "torch.Size([128, 23618])\n",
            "torch.Size([128, 24140])\n",
            "torch.Size([128, 25326])\n",
            "torch.Size([128, 25852])\n",
            "torch.Size([128, 27320])\n",
            "torch.Size([128, 28448])\n",
            "torch.Size([128, 29813])\n",
            "torch.Size([128, 22922])\n",
            "torch.Size([128, 23535])\n",
            "torch.Size([128, 24609])\n",
            "torch.Size([128, 25240])\n",
            "torch.Size([128, 21341])\n",
            "torch.Size([128, 25675])\n",
            "torch.Size([128, 23753])\n",
            "torch.Size([128, 24925])\n",
            "torch.Size([128, 25786])\n",
            "torch.Size([128, 27589])\n",
            "torch.Size([128, 22386])\n",
            "torch.Size([128, 28606])\n",
            "torch.Size([128, 25860])\n",
            "torch.Size([128, 25355])\n",
            "torch.Size([128, 32616])\n",
            "torch.Size([128, 31954])\n",
            "torch.Size([128, 27881])\n",
            "torch.Size([128, 22469])\n",
            "torch.Size([128, 22353])\n",
            "torch.Size([128, 25545])\n",
            "torch.Size([128, 27894])\n",
            "torch.Size([128, 25400])\n",
            "torch.Size([128, 30304])\n",
            "torch.Size([128, 25229])\n",
            "torch.Size([128, 21738])\n",
            "torch.Size([128, 23334])\n",
            "torch.Size([128, 26036])\n",
            "torch.Size([128, 14997])\n",
            "torch.Size([128, 25775])\n",
            "torch.Size([128, 24201])\n",
            "torch.Size([128, 23059])\n",
            "torch.Size([128, 27420])\n",
            "torch.Size([128, 30539])\n",
            "torch.Size([128, 22832])\n",
            "torch.Size([128, 23737])\n",
            "torch.Size([128, 25334])\n",
            "torch.Size([128, 24979])\n",
            "torch.Size([128, 27501])\n",
            "torch.Size([128, 26386])\n",
            "torch.Size([128, 22397])\n",
            "torch.Size([128, 26058])\n",
            "torch.Size([128, 32049])\n",
            "torch.Size([128, 25187])\n",
            "torch.Size([128, 18764])\n",
            "torch.Size([128, 24071])\n",
            "torch.Size([128, 16996])\n",
            "torch.Size([128, 23455])\n",
            "torch.Size([128, 22511])\n",
            "torch.Size([128, 26063])\n",
            "torch.Size([128, 10267])\n",
            "torch.Size([128, 27877])\n",
            "torch.Size([128, 24852])\n",
            "torch.Size([128, 26513])\n",
            "torch.Size([128, 20377])\n",
            "torch.Size([128, 30493])\n",
            "torch.Size([128, 20965])\n",
            "torch.Size([128, 22134])\n",
            "torch.Size([128, 15465])\n",
            "torch.Size([128, 23097])\n",
            "torch.Size([128, 29624])\n",
            "torch.Size([128, 26439])\n",
            "torch.Size([128, 24308])\n",
            "torch.Size([128, 23138])\n",
            "torch.Size([128, 27277])\n",
            "torch.Size([128, 22663])\n",
            "torch.Size([128, 20385])\n",
            "torch.Size([128, 30169])\n",
            "torch.Size([128, 31424])\n",
            "torch.Size([128, 30260])\n",
            "torch.Size([128, 18368])\n",
            "torch.Size([128, 18389])\n",
            "torch.Size([128, 23489])\n",
            "torch.Size([128, 22912])\n",
            "torch.Size([128, 20815])\n",
            "torch.Size([128, 25864])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxlEUDlC_5y"
      },
      "source": [
        "import IPython.display\n",
        "\n",
        "# cello_c_chord.write('cello-C-chord.mid')\n",
        "pm = piano_roll_to_pretty_midi(train_dataset[2])\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pdqrt0uFqn6"
      },
      "source": [
        "#were in bussiness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "IBO96-EtFuY4"
      },
      "source": [
        "#@title KWTA\n",
        "\n",
        "\n",
        "class SparsifyBase(nn.Module):\n",
        "    def __init__(self, sparse_ratio=0.5):\n",
        "        super(SparsifyBase, self).__init__()\n",
        "        self.sr = sparse_ratio\n",
        "        self.preact = None\n",
        "        self.act = None\n",
        "    def get_activation(self):\n",
        "        def hook(model, input, output):\n",
        "            self.preact = input[0].cpu().detach().clone()\n",
        "            self.act = output.cpu().detach().clone()\n",
        "        return hook\n",
        "    def record_activation(self):\n",
        "        self.register_forward_hook(self.get_activation())\n",
        "\n",
        "\n",
        "class Sparsify1D_kactive(SparsifyBase):\n",
        "    def __init__(self, k=1):\n",
        "        super(Sparsify1D_kactive, self).__init__()\n",
        "        self.k = k\n",
        "    def forward(self, x):\n",
        "        m = torch.zeros(x.shape).to(device)\n",
        "        for i in range(self.k):\n",
        "            indeces = x.topk(self.k, dim=1)[1][:, i]\n",
        "            m += torch.mul(torch.zeros(x.shape).to(device).scatter(1, indeces.unsqueeze(1), 1), x)\n",
        "            # print(\"\\n hi\", m )\n",
        "        return m.double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "xgpIeeomFwls"
      },
      "source": [
        "#@title TCN \n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 1)\n",
        "        self.conv2.weight.data.normal_(0, 1)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"block \", x.size())\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "        # print(\"last layer conv\", self.network[-1].conv2.weight.data[:,0,:].size())\n",
        "        # print(\"last layer conv\", self.network[-1].conv2.weight.data[:,0,:])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Onb8eMTOFzUz"
      },
      "source": [
        "#@title TCN - Autoeconder \n",
        "\n",
        "class TCNAutoencoder(nn.Module):\n",
        "    def __init__(self, kernel_size, dropout, wta_k):\n",
        "        super(TCNAutoencoder, self).__init__()\n",
        "        self.wta = Sparsify1D_kactive(k = wta_k)\n",
        "        self.feature = TemporalConvNet(128, [64*3,64*4], kernel_size, dropout=dropout).double()\n",
        "        self.encoder = torch.nn.Conv1d(in_channels=128, out_channels=64*4*8, kernel_size=kernel_size, padding=0, bias=True, stride=4)\n",
        "        self.decoder = torch.nn.ConvTranspose1d(in_channels=64*4*8, out_channels=128, kernel_size=kernel_size, padding=0, bias=True, stride=4)\n",
        "        # self.encoder.weight.data.normal_(30)\n",
        "        # self.decoder.weight.data.normal_(300)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.code = None\n",
        "        # torch.nn.init.xavier_uniform(self.encoder.weight)\n",
        "        # torch.nn.init.xavier_uniform(self.decoder.weight)\n",
        "    def get_kernels(self):\n",
        "        return self.decoder.weight.data[:,0,:]\n",
        "    def feature_map(self, x):\n",
        "        code = self.code\n",
        "        return code\n",
        "    def forward(self, x):\n",
        "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
        "        # output = self.feature(x)\n",
        "        self.code = self.wta(self.encoder(x))\n",
        "        # print(\"code\",self.code.shape)\n",
        "        output = self.decoder(self.code )\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "yrrRt_sTF1TL"
      },
      "source": [
        "#@title GO\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "model = TCNAutoencoder(kernel_size=4, \n",
        "                       dropout=0.2, \n",
        "                       wta_k = 3).to(device).double()\n",
        "print(\"TCNAutoencoder trainable parameters: \", get_model_parameters(model))\n",
        "\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.MSELoss().to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=.01, weight_decay = 0.00001, momentum=0.05) ##this has weight decay just like you implemented\n",
        "optimizer = optim.AdamW(model.parameters(), lr=.005,  betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True) ##this has weight decay just like you implemented\n",
        "epochs = 30\n",
        "history = {\"loss\": []}\n",
        "\n",
        "calc = []\n",
        "total_len = 0\n",
        "for i in range(epochs):\n",
        "    #decaying WTA\n",
        "    if i % 5 == 0 and i != 0:\n",
        "        model.wta.k = max(1, model.wta.k - 1)\n",
        "        print(\"model.wta.k: \", model.wta.k)\n",
        "\n",
        "    # sample_size = 1000\n",
        "    # raw_input = (train_dataset[0] - 224.5851314855734) / 111.61066023994307\n",
        "    # raw_input = raw_input[None, :, 0:sample_size].to(device).double()\n",
        "    # model_out = model(raw_input)[0]\n",
        "    # model_out = (model_out * 111.61066023994307) + 224.5851314855734\n",
        "    # pm = piano_roll_to_pretty_midi(model_out.cpu().detach().numpy().astype(int))\n",
        "    # IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n",
        "    for train_data in train_dataset:\n",
        "        # calc.extend(train_data.flatten().numpy())\n",
        "        #normalize \n",
        "        train_data = (train_data - 224.15541543187527) / 111.14747885919755\n",
        "        #preprocess\n",
        "        lenby4 = len(train_data) // 4\n",
        "        train_data = train_data[None, 0:lenby4*4].to(device).double()\n",
        "\n",
        "        #preprocess\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_data)\n",
        "\n",
        "        loss = loss_fn(output, train_data[:,:,0:output.shape[2]])\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        history[\"loss\"].append(float(loss)*lenby4)\n",
        "        print(\"Loss : {} \".format(float(loss)))\n",
        "        total_len += lenby4\n",
        "    print(\"Epoch : {} \\t Loss : {} \".format(i, round(float(np.mean(history[\"loss\"], axis=0)/total_len),7)))\n",
        "    history[\"loss\"] = []\n",
        "    total_len = 0\n",
        "\n",
        "# print(len(calc))\n",
        "# print(np.mean(calc, axis=0))\n",
        "# print(np.std(calc, axis=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peJ0ArTRO441"
      },
      "source": [
        "index_example = 1\n",
        "size = 1000\n",
        "\n",
        "#get raw_input\n",
        "raw_input = train_dataset[index_example][:,0:size]\n",
        "\n",
        "print(raw_input.shape)\n",
        "print(raw_input.max())\n",
        "\n",
        "\n",
        "print(\"orginal 1\")\n",
        "\n",
        "# cello_c_chord.write('cello-C-chord.mid')\n",
        "pm = piano_roll_to_pretty_midi(raw_input.numpy().astype(int))\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6VwoEqgOpIB"
      },
      "source": [
        "print(\"reconstructed\")\n",
        "name = \"music_rec\"\n",
        "\n",
        "raw_input = (raw_input - 224.5851314855734) / 111.61066023994307\n",
        "\n",
        "raw_input = raw_input[None, 0:(len(raw_input) // 4)*4].to(device).double()[:,0:size]\n",
        "print(\"train_data size\", train_data.shape)\n",
        "\n",
        "model_out = model(raw_input)[0]\n",
        "\n",
        "model_out = (model_out * 111.61066023994307) + 224.5851314855734\n",
        "# model_out[model_out < 0] = 0\n",
        "\n",
        "print(\"model_out size\", model_out.shape)\n",
        "print(\"model_out max\", model_out.max())\n",
        "\n",
        "# print(model_out.cpu().detach().numpy().astype(int))\n",
        "model_out = model_out.cpu().detach().numpy().astype(int)\n",
        "\n",
        "# model_out = model_out[model_out>=0]\n",
        "print(\"model_out size\", model_out)\n",
        "\n",
        "pm = piano_roll_to_pretty_midi(model_out)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUgS7p5HhZLX"
      },
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "from google.colab import files\n",
        "files.download('model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}