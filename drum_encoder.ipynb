{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drum_encoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2MKHyYd6v76yptN3PFeHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/shift_invariant_dictionary_learning/blob/main/drum_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-Ahmt_6UN4"
      },
      "source": [
        "!pip install skorch\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwgNMrY06eZg"
      },
      "source": [
        "# Plot Utils "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbAc2yigmeEi"
      },
      "source": [
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJAJnHme7HIv"
      },
      "source": [
        "# Generate some drum synthetic data similar to symbolic music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo4KzUgW7Gtp",
        "outputId": "5c3955e3-0359-49e5-9866-ac5e4df4ffb6"
      },
      "source": [
        "# lets make the data 400 in lenghth to match autoencoder imlementation \n",
        "#lets have 10 different sections each 40 in lenght -- ideal conditions \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## basic patters\n",
        "downbeat = [1 if i % 4 ==0 else 0 for i in range(0,40)]\n",
        "downbeat2x = [1 if i % 2 ==0 else 0 for i in range(0,40)]\n",
        "\n",
        "high_hats = [2 if i % 4 ==1 else 0 for i in range(0,40)]\n",
        "high_hats2x = [2 if i % 2 ==1 else 0 for i in range(0,40)]\n",
        "\n",
        "tom_drum = [3 if i % 4 ==2 else 0 for i in range(0,40)]\n",
        "\n",
        "## combine basic patters\n",
        "\n",
        "comb1 = np.array(downbeat) + np.array(high_hats)  \n",
        "comb2 = np.array(downbeat) + np.array(high_hats2x)  \n",
        "comb3 = np.array(downbeat2x) + np.array(high_hats2x)  \n",
        "comb4 = np.array(downbeat) + np.array(high_hats2x) + np.array(tom_drum)\n",
        "comb5 = np.array(downbeat2x) + np.array(high_hats)\n",
        "\n",
        "\n",
        "print(downbeat)\n",
        "print(downbeat2x)\n",
        "print(high_hats)\n",
        "print(high_hats2x)\n",
        "print(tom_drum, \"\\n\")\n",
        "\n",
        "print(list(comb1))\n",
        "print(list(comb2))\n",
        "print(list(comb3))\n",
        "print(list(comb4))\n",
        "print(list(comb5))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
            "[0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0]\n",
            "[0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2]\n",
            "[0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0] \n",
            "\n",
            "[1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0]\n",
            "[1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2]\n",
            "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
            "[1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2]\n",
            "[1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyknkQtW6iaq"
      },
      "source": [
        "# Auto Econder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC8m2nGD6htU"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = torch.nn.Conv1d(in_channels=1, out_channels=10, kernel_size=40, padding=0, bias=False, stride=40)\n",
        "        self.decoder = torch.nn.ConvTranspose1d(in_channels=10, out_channels=1, kernel_size=40, padding=0, bias=False, stride=40)\n",
        "    def get_kernels(self):\n",
        "        return self.encoder.weight.data[:,0,:]\n",
        "    def feature_map(self, x):\n",
        "        code = self.encoder(x)\n",
        "        return code\n",
        "    def forward(self, x):\n",
        "        code = self.encoder(x)\n",
        "        reconstruct = self.decoder(code)\n",
        "        return reconstruct\n",
        "\n",
        "\n",
        "\n",
        "model = autoencoder()\n",
        "inputs_size = torch.tensor(X_train).float()\n",
        "out_size = model(inputs)\n",
        "\n",
        "print(inputs_size.size())\n",
        "print(out_size.size())\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "loss_fn = torch.nn.L1Loss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=.05, weight_decay = 0.00001, momentum=0.05) ##this has weight decay just like you implemented\n",
        "print(inputs.size())\n",
        "\n",
        "epochs = 12000\n",
        "history = {\"loss\": []}\n",
        "for i in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(inputs)\n",
        "  # print(output.size())\n",
        "  loss = loss_fn(output, inputs)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  history[\"loss\"].append(float(loss))\n",
        "  if i % 1000 == 0: print(\"Epoch : {} \\t Loss : {} \\t \".format(i, round(float(loss),4)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}