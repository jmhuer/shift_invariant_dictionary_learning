{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyND0Zj3SQsibS5bSTaccJat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/shift_invariant_dictionary_learning/blob/main/HTwAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "35aba2d2-c368-4c9a-d9cb-ce3778ea6961"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/jmhuer/ModularSparseAutoencoder\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "!git clone https://github.com/jmhuer/HT\n",
        "!git clone https://github.com/Tsung-Ping/functional-harmony\n",
        "# %cd /content/POP909-Dataset/data_process\n",
        "!pip install libfmp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=d7a49ccab7974786c73882c765d14d3ca496463a382e00cad5f234b779f40838\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'ModularSparseAutoencoder'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 12), reused 10 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 16.19 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Cloning into 'HT'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 26 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n",
            "Cloning into 'functional-harmony'...\n",
            "remote: Enumerating objects: 460, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 460 (delta 13), reused 49 (delta 12), pack-reused 396\u001b[K\n",
            "Receiving objects: 100% (460/460), 2.50 MiB | 27.81 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "Collecting libfmp\n",
            "  Downloading libfmp-1.2.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 15.0 MB/s \n",
            "\u001b[?25hCollecting pysoundfile<1.0.0,>=0.9.0\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numba<1.0.0,>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.51.2)\n",
            "Collecting music21<6.0.0,>=5.7.0\n",
            "  Downloading music21-5.7.2.tar.gz (18.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.5 MB 343 kB/s \n",
            "\u001b[?25hCollecting ipython<8.0.0,>=7.8.0\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (3.2.2)\n",
            "Requirement already satisfied: librosa<1.0.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.19.5)\n",
            "Requirement already satisfied: pretty-midi<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.2.9)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.1.5)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.4.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.18.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (57.4.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.1.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 88.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.8.0->libfmp) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (21.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.22.2.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.0.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.1.0->libfmp) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1.0.0,>=0.51.0->libfmp) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->libfmp) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.8.0->libfmp) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.4.4)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty-midi<1.0.0,>=0.2.0->libfmp) (1.2.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.8.0->libfmp) (0.2.5)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile<1.0.0,>=0.9.0->libfmp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile<1.0.0,>=0.9.0->libfmp) (2.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.24.3)\n",
            "Building wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-5.7.2-py3-none-any.whl size=22024624 sha256=2c44584c64e2bb568191bf5ee383a07983536079f0264e66af207311e357c5e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cb/ae/fd264ebf1e9cf01c15576ee4c128f1bfd907a120c0a7a5b542\n",
            "Successfully built music21\n",
            "Installing collected packages: prompt-toolkit, pysoundfile, music21, ipython, libfmp\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: music21\n",
            "    Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.28.0 libfmp-1.2.1 music21-5.7.2 prompt-toolkit-3.0.20 pysoundfile-0.9.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km3TD1zn3j3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88a6f2d-6335-4fe0-d1a7-5ba0946ce72d"
      },
      "source": [
        "from HT.BPS_FH_preprocessing import main\n",
        "\n",
        "main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: load note data ...\n",
            "lowest pitch = 24 highest pitch =  101\n",
            "Message: load chord labels...\n",
            "Message: get framewise labels ...\n",
            "max_length = 8482\n",
            "min_length = 872\n",
            "keys in corpus['op'] = dict_keys(['pianoroll', 'chromagram', 'start_time', 'label'])\n",
            "label fields =  [('op', '<U10'), ('onset', '<f8'), ('key', '<U10'), ('degree1', '<U10'), ('degree2', '<U10'), ('quality', '<U10'), ('inversion', '<i8'), ('rchord', '<U10'), ('root', '<U10'), ('tquality', '<U10'), ('chord_change', '<i8')]\n",
            "Running Message: augment data...\n",
            "keys in corpus_aug['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label'])\n",
            "Running Message: reshape data...\n",
            "keys in corpus_aug_reshape['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label', 'len'])\n",
            "sequence_len_non_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "sequence_len_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "Preprocessed data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "JdRQ7nWU1zIZ",
        "outputId": "c74cb912-d7d4-4b73-90c3-cca70955aebc"
      },
      "source": [
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "from HT.BPS_FH_preprocessing import main\n",
        "from HT.chord_symbol_recognition import load_data_symbol\n",
        "#\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "##only tensor transforms\n",
        "\n",
        "\n",
        "datasetpath = '/content/'\n",
        "\n",
        "train_data, test_data = load_data_symbol(dir=datasetpath + 'BPS_FH_preprocessed_data_MIREX_Mm.pickle', test_set_id=1, sequence_with_overlap=True)\n",
        "print(\"load_data_symbol train_data size: {}\".format(train_data[\"tchord\"].shape))\n",
        "\n",
        "\n",
        "\n",
        "# dataset = load_data(train_data, test_data, batch_size=16)\n",
        "\n"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load chord symbol data...\n",
            "test_set_id = 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-271-cfb18750d097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdatasetpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'BPS_FH_preprocessed_data_MIREX_Mm.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_with_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_data_symbol train_data size: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tchord\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/HT/chord_symbol_recognition.py\u001b[0m in \u001b[0;36mload_data_symbol\u001b[0;34m(dir, test_set_id, sequence_with_overlap)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcorpus_aug_reshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keys in corpus_aug_reshape[\\'shift_id\\'][\\'op\\'] ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_aug_reshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shift_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mshift_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_aug_reshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yULLsZoeq6eV"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import random\n",
        "\n",
        "\n",
        "class autoencoder:\n",
        "    def __init__(self):\n",
        "        self.weights = {\n",
        "            'encoder_h1': tf.Variable(tf.random_normal([88, 20])/100),\n",
        "            'encoder_h2': tf.Variable(tf.random_normal([4000, 1000])/100),\n",
        "            'decoder_h1': tf.Variable(tf.random_normal([1000, 2000])/100),\n",
        "            'decoder_h2': tf.Variable(tf.random_normal([20, 88])/100)\n",
        "        }\n",
        "        self.biases = {\n",
        "            'encoder_b1': tf.Variable(tf.random_normal([1000])),\n",
        "            'decoder_b1': tf.Variable(tf.random_normal([11264]))\n",
        "        }\n",
        "        # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer_1 = tf.nn.relu(tf.matmul(x, self.weights['encoder_h1']))\n",
        "        return layer_1\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        layer_2 = tf.nn.sigmoid(tf.matmul(x, self.weights['decoder_h2']))\n",
        "        return layer_2\n",
        "    def code(self, x):\n",
        "        # do =  x[0,:,:]\n",
        "        # x = tf.layers.flatten(x)\n",
        "        encoder_op = self.encoder(x[0,:,:])\n",
        "        decoder_op = self.decoder(encoder_op)[None]\n",
        "        # x = tf.zeros((40, 128,88))\n",
        "        decoder_op = tf.placeholder_with_default(decoder_op,[None, 128, 88])\n",
        "\n",
        "        # # print(\"decoder x shape \",x.shape)\n",
        "        # # self.save(tf.transpose(tf.identity(x)[0,:,:]))\n",
        "        # print(\"xshape\", x.shape)\n",
        "        # x_p = tf.placeholder(tf.int32, [None, hp.n_steps, 88], name=\"pianoroll\")\n",
        "        # tf.assign(x_p,decoder_op)\n",
        "        # # with tf.Session() as sess:\n",
        "        # #     o = sess.run(x_p.assign(decoder_op)) # == 5.0, using the Tensor value\n",
        "        #     print(\"o shape\", o)\n",
        "        #     # sess.run(x_p, {feed_dict={x_p: o.astype(int)})\n",
        "        return decoder_op\n",
        "\n",
        "\n",
        "# zeros = tf.zeros((1,128,88))\n",
        "# ae = autoencoder()\n",
        "# ae.code(zeros)\n",
        "# ae.reload()\n",
        "\n",
        "\n"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BA7MRaf3nrC"
      },
      "source": [
        "# This preprocesses data and creates pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dDuXiM626TA"
      },
      "source": [
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "\n",
        "\n",
        "# Chord symbol recognition\n",
        "# train_BTC() # Bi-directional Transformer for Chord Recognition\n",
        "# train_CRNN() # Convolutional Recurrent Neural Network\n",
        "root_dict = {'C': 0, 'C+': 1, 'D': 2, 'D+': 3, 'E': 4, 'F': 5, 'F+': 6, 'G': 7, 'G+': 8, 'A': 9, 'A+': 10, 'B': 11, 'pad': 12}\n",
        "tquality_dict = {'M': 0, 'm': 1, 'O': 2, 'pad': 3}  # 'O' stands for 'others'\n",
        "n_chord_classes = 24 + 1  # 24 major-minor modes plus 1 others\n",
        "\n",
        "ae = autoencoder()\n",
        "# Hyperparameters\n",
        "hyperparameters = namedtuple('hyperparameters',\n",
        "                              ['dataset',\n",
        "                              'test_set_id',\n",
        "                              'graph_location',\n",
        "                              'n_root_classes',\n",
        "                              'n_tquality_classes',\n",
        "                              'n_chord_classes',\n",
        "                              'n_steps',\n",
        "                              'input_embed_size',\n",
        "                              'n_layers',\n",
        "                              'n_heads',\n",
        "                              'train_sequence_with_overlap',\n",
        "                              'initial_learning_rate',\n",
        "                              'drop',\n",
        "                              'n_batches',\n",
        "                              'n_training_steps',\n",
        "                              'n_in_succession',\n",
        "                              'annealing_rate',\n",
        "                               'autoencoder'])\n",
        "\n",
        "hp = hyperparameters(dataset='/content/', # {'BPS_FH', 'Preludes'}\n",
        "                      test_set_id=1, # {1, 2, 3, 4}\n",
        "                      graph_location='model',\n",
        "                      n_root_classes=len(root_dict.keys()),\n",
        "                      n_tquality_classes=len(tquality_dict.keys()),\n",
        "                      n_chord_classes=n_chord_classes,\n",
        "                      n_steps=128,\n",
        "                      input_embed_size=128,\n",
        "                      n_layers=2,\n",
        "                      n_heads=4,\n",
        "                      train_sequence_with_overlap=True,\n",
        "                      initial_learning_rate=1e-4,\n",
        "                      drop=0.1,\n",
        "                      n_batches=1,\n",
        "                      n_training_steps=100000*2,\n",
        "                      n_in_succession=10,\n",
        "                      annealing_rate=1.1,\n",
        "                      autoencoder=ae)"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrXxS30i3ygJ"
      },
      "source": [
        "# Make pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NJTCEIzM1Rw",
        "outputId": "04b5c7fc-9f6c-4c98-f537-a9e4503a114f"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFtAeNaVNjDW",
        "outputId": "073f930e-8ecd-42e4-c193-17da102faa24"
      },
      "source": [
        "train_HT(hp, train_data, test_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run HT chord recognition on /content/-1...\n",
            "n_train_sequences = 54320\n",
            "n_test_sequences = 294\n",
            "n_iterations_per_epoch = 54320\n",
            "hyperparameters(dataset='/content/', test_set_id=1, graph_location='model', n_root_classes=13, n_tquality_classes=4, n_chord_classes=25, n_steps=128, input_embed_size=128, n_layers=2, n_heads=4, train_sequence_with_overlap=True, initial_learning_rate=0.0001, drop=0.1, n_batches=1, n_training_steps=200000, n_in_succession=10, annealing_rate=1.1, autoencoder=<__main__.autoencoder object at 0x7f45264bf050>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:393: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:263: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  warnings.warn('`tf.layers.conv1d` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss ae Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"optimization/gradients/model/input_embedding_regionalization/map_1/while/GatherV2_grad/Reshape_1:0\", shape=(?,), dtype=int32), values=Tensor(\"optimization/gradients/model/input_embedding_regionalization/map_1/while/GatherV2_grad/Reshape:0\", shape=(?, ?), dtype=float32), dense_shape=Tensor(\"optimization/gradients/model/input_embedding_regionalization/map_1/while/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving graph to: model\n",
            "Train the model...\n",
            "()\n",
            "*~ loss_cc 2.2763, loss_tc 2.5781 ~*, loss_ae 0.1250 ~*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOAc_Zu9V2e7"
      },
      "source": [
        "#test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9CkJV5MthTU"
      },
      "source": [
        "#play example from BPS dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=8, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxlEUDlC_5y"
      },
      "source": [
        "import IPython.display\n",
        "index = 54 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "pianoex = test_data[\"pianoroll\"]\n",
        "# for i,ba in enumerate(pianoex): \n",
        "#     ##print(ba[0].shape)\n",
        "#     if i < index: \n",
        "#         continue\n",
        "#     listen = i\n",
        "\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "print(pianoex[index].T.shape)\n",
        "arr = pad88to128(pianoex[index].T)\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN2XoiSO2wbm"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsB6hR7-w3JR"
      },
      "source": [
        "import IPython.display\n",
        "index = 54 ## 44 66 & 0 & 1 500 omg 1021\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#lets play a batch \n",
        "pianoex = train_data[\"pianoroll\"]\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "m = np.array(pianoex[index][None].astype(\"float32\"))\n",
        "\n",
        "\n",
        "\n",
        "g = tf.train.import_meta_graph('/content/mymymy.chkpt.meta')\n",
        "print(g)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "with tf.compat.v1.Session() as sess: \n",
        "    init.run() \n",
        "    g.restore(sess, model_checkpoint)\n",
        "    out = hp.autoencoder.code(m)\n",
        "    s = 0.5* tf.reduce_mean(tf.pow(out - m, 2))\n",
        "    np_out = sess.run(out)\n",
        "    ss = sess.run(s)\n",
        "\n",
        "print(\"MSE: \", ss)\n",
        "\n",
        "np_out = (np_out>=.95) \n",
        "np_out = np_out[0,:,:].T\n",
        "\n",
        "arr = pad88to128(np_out)\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qAXyG_NCYn4"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}