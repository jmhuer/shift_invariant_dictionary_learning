{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "transformer_tutorial.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJ5sz2YF3Sp"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "3e90840c-e653-4e09-d1cf-27355c6bb726"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/jmhuer/ModularSparseAutoencoder\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "!git clone https://github.com/jmhuer/HT\n",
        "!git clone https://github.com/Tsung-Ping/functional-harmony\n",
        "# %cd /content/POP909-Dataset/data_process\n",
        "!pip install libfmp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=1f739b606a285929417de3e218409032d221bbe0aa0425ce549e57730bb2b118\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'ModularSparseAutoencoder'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 12), reused 10 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 24.36 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Cloning into 'HT'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 26 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n",
            "Cloning into 'functional-harmony'...\n",
            "remote: Enumerating objects: 460, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 460 (delta 13), reused 49 (delta 12), pack-reused 396\u001b[K\n",
            "Receiving objects: 100% (460/460), 2.50 MiB | 27.21 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "Collecting libfmp\n",
            "  Downloading libfmp-1.2.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.4.1)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.1.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (3.2.2)\n",
            "Collecting ipython<8.0.0,>=7.8.0\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 82.2 MB/s \n",
            "\u001b[?25hCollecting pysoundfile<1.0.0,>=0.9.0\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.19.5)\n",
            "Requirement already satisfied: numba<1.0.0,>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.51.2)\n",
            "Requirement already satisfied: librosa<1.0.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.8.1)\n",
            "Collecting music21<6.0.0,>=5.7.0\n",
            "  Downloading music21-5.7.2.tar.gz (18.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.5 MB 336 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pretty-midi<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.2.9)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (57.4.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.18.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.8.0->libfmp) (0.8.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.22.2.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (21.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.0.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.10.3.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.1.0->libfmp) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1.0.0,>=0.51.0->libfmp) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->libfmp) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.8.0->libfmp) (0.7.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.23.0)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty-midi<1.0.0,>=0.2.0->libfmp) (1.2.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.8.0->libfmp) (0.2.5)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile<1.0.0,>=0.9.0->libfmp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile<1.0.0,>=0.9.0->libfmp) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.10)\n",
            "Building wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-5.7.2-py3-none-any.whl size=22024624 sha256=844107616eaf1dd2fa21ad8a3d843571ef976c6c8fe241f0f227c3b9b606c28c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cb/ae/fd264ebf1e9cf01c15576ee4c128f1bfd907a120c0a7a5b542\n",
            "Successfully built music21\n",
            "Installing collected packages: prompt-toolkit, pysoundfile, music21, ipython, libfmp\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: music21\n",
            "    Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.28.0 libfmp-1.2.1 music21-5.7.2 prompt-toolkit-3.0.20 pysoundfile-0.9.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3uwo-E8NFshn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab39520-a2ae-45aa-8376-a4e531398eb6"
      },
      "source": [
        "#@title Pytorch for DL\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "\n",
        "def get_model_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BA7MRaf3nrC"
      },
      "source": [
        "# This preprocesses data and creates pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dDuXiM626TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9721c8a6-3eac-4fb1-bc10-4a6f9400f9da"
      },
      "source": [
        "\n",
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "\n",
        "\n",
        "# Chord symbol recognition\n",
        "# train_BTC() # Bi-directional Transformer for Chord Recognition\n",
        "# train_CRNN() # Convolutional Recurrent Neural Network\n",
        "root_dict = {'C': 0, 'C+': 1, 'D': 2, 'D+': 3, 'E': 4, 'F': 5, 'F+': 6, 'G': 7, 'G+': 8, 'A': 9, 'A+': 10, 'B': 11, 'pad': 12}\n",
        "tquality_dict = {'M': 0, 'm': 1, 'O': 2, 'pad': 3}  # 'O' stands for 'others'\n",
        "n_chord_classes = 24 + 1  # 24 major-minor modes plus 1 others\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "hyperparameters = namedtuple('hyperparameters',\n",
        "                              ['dataset',\n",
        "                              'test_set_id',\n",
        "                              'graph_location',\n",
        "                              'n_root_classes',\n",
        "                              'n_tquality_classes',\n",
        "                              'n_chord_classes',\n",
        "                              'n_steps',\n",
        "                              'input_embed_size',\n",
        "                              'n_layers',\n",
        "                              'n_heads',\n",
        "                              'train_sequence_with_overlap',\n",
        "                              'initial_learning_rate',\n",
        "                              'drop',\n",
        "                              'n_batches',\n",
        "                              'n_training_steps',\n",
        "                              'n_in_succession',\n",
        "                              'annealing_rate'])\n",
        "\n",
        "hp = hyperparameters(dataset='/content/', # {'BPS_FH', 'Preludes'}\n",
        "                      test_set_id=1, # {1, 2, 3, 4}\n",
        "                      graph_location='model',\n",
        "                      n_root_classes=len(root_dict.keys()),\n",
        "                      n_tquality_classes=len(tquality_dict.keys()),\n",
        "                      n_chord_classes=n_chord_classes,\n",
        "                      n_steps=128,\n",
        "                      input_embed_size=128,\n",
        "                      n_layers=2,\n",
        "                      n_heads=4,\n",
        "                      train_sequence_with_overlap=True,\n",
        "                      initial_learning_rate=1e-4,\n",
        "                      drop=0.1,\n",
        "                      n_batches=40,\n",
        "                      n_training_steps=100000,\n",
        "                      n_in_succession=10,\n",
        "                      annealing_rate=1.1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km3TD1zn3j3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c822fe-508d-400f-d04e-ea4e750b0238"
      },
      "source": [
        "from HT.BPS_FH_preprocessing import main\n",
        "\n",
        "main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: load note data ...\n",
            "lowest pitch = 24 highest pitch =  101\n",
            "Message: load chord labels...\n",
            "Message: get framewise labels ...\n",
            "max_length = 8482\n",
            "min_length = 872\n",
            "keys in corpus['op'] = dict_keys(['pianoroll', 'chromagram', 'start_time', 'label'])\n",
            "label fields =  [('op', '<U10'), ('onset', '<f8'), ('key', '<U10'), ('degree1', '<U10'), ('degree2', '<U10'), ('quality', '<U10'), ('inversion', '<i8'), ('rchord', '<U10'), ('root', '<U10'), ('tquality', '<U10'), ('chord_change', '<i8')]\n",
            "Running Message: augment data...\n",
            "keys in corpus_aug['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label'])\n",
            "Running Message: reshape data...\n",
            "keys in corpus_aug_reshape['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label', 'len'])\n",
            "sequence_len_non_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "sequence_len_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "Preprocessed data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrXxS30i3ygJ"
      },
      "source": [
        "# Make pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRQ7nWU1zIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c9213f-1f1a-4a07-c6a3-853c2d17cc54"
      },
      "source": [
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "from HT.BPS_FH_preprocessing import main\n",
        "from HT.chord_symbol_recognition import load_data_symbol\n",
        "#\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "class BPS(Dataset):\n",
        "    def __init__(self, data):\n",
        "        \"todo: transpose\"\n",
        "        self.all_data = []\n",
        "        temp = []\n",
        "        self.label = []\n",
        "        temp2 = []\n",
        "        self.chord_change = []\n",
        "        temp3 = []      \n",
        "        i = 0 \n",
        "        for pi,chord, cc in zip(data[\"pianoroll\"],data[\"tchord\"],data['label']['chord_change']):\n",
        "            temp.append(torch.tensor(pi).double())\n",
        "            temp2.append(torch.tensor(chord).double())\n",
        "            temp3.append(torch.tensor(cc).double())\n",
        "            i+=1\n",
        "            if i > 30000: break\n",
        "        self.all_data = torch.cat(temp)\n",
        "        self.label = torch.cat(temp2)\n",
        "        self.chord_change = torch.cat(temp3)\n",
        "        print(\"data\" , self.all_data.shape)\n",
        "        print(\"label\" , self.label.shape)\n",
        "        ##time.sleep(120)\n",
        "        self.transform = self.make_transform()\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "    def __getitem__(self, idx):\n",
        "        piano_roll_slice = self.all_data[idx,:]\n",
        "        label_slice = self.label[idx]\n",
        "        chord_change_slice = self.chord_change[idx]\n",
        "        tranformed_piano_roll_slice = self.transform[\"norm\"](piano_roll_slice[None][None])[0,0,:]\n",
        "        #print(tranformed_piano_roll_slice.shape)\n",
        "        #print(\"label_slice\", label_slice)\n",
        "        return tranformed_piano_roll_slice, label_slice, chord_change_slice\n",
        "    def make_transform(self):\n",
        "        mean = self.all_data.mean()\n",
        "        std = self.all_data.std() * 2\n",
        "        # print(\"mean \", mean.shape)\n",
        "        # print(\"std \", std.shape)\n",
        "        tensor_transform = {\n",
        "          'norm':\n",
        "              transforms.Compose([\n",
        "                  transforms.Normalize([mean], [std])  # Imagenet standards\n",
        "              ]),\n",
        "          \"inverse_norm\":\n",
        "                transforms.Normalize(\n",
        "                  mean= [-m/s for m, s in zip([mean], [std])],\n",
        "                  std= [1/s for s in [std]]\n",
        "                )\n",
        "            }\n",
        "        return tensor_transform\n",
        "\n",
        "\n",
        "def load_data(train_data, test_data,  num_workers=0, batch_size=16, random_seed = 40):\n",
        "    '''\n",
        "    this data loading proccedure assumes dataset/train/ dataset/val/ folders\n",
        "    also assumes transform dictionary with train and val\n",
        "    '''\n",
        "    dataset_train = BPS(train_data) \n",
        "    dataset_val = BPS(test_data) \n",
        "\n",
        "    print(\"Size of train dataset: \",len(dataset_train))\n",
        "    print(\"Size of val dataset: \",len(dataset_val))\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(dataset_train, batch_size=batch_size, shuffle=False, drop_last=True),\n",
        "        'val': DataLoader(dataset_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    }\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "\n",
        "##only tensor transforms\n",
        "\n",
        "\n",
        "\n",
        "train_data, test_data = load_data_symbol(dir=hp.dataset + 'BPS_FH_preprocessed_data_MIREX_Mm.pickle', test_set_id=hp.test_set_id, sequence_with_overlap=hp.train_sequence_with_overlap)\n",
        "print(\"load_data_symbol train_data size: {}\".format(train_data[\"tchord\"].shape))\n",
        "\n",
        "\n",
        "\n",
        "mydataset = load_data(train_data, test_data, batch_size=16)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load chord symbol data...\n",
            "test_set_id = 1\n",
            "keys in corpus_aug_reshape['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label', 'len'])\n",
            "shift_list = ['shift_-1', 'shift_-2', 'shift_-3', 'shift_0', 'shift_1', 'shift_2', 'shift_3', 'shift_4', 'shift_5', 'shift_6']\n",
            "train_op_list = ['2', '3', '4', '6', '7', '8', '10', '11', '12', '14', '15', '16', '18', '19', '20', '22', '23', '24', '26', '27', '28', '30', '31', '32']\n",
            "test_op_list = ['1', '5', '9', '13', '17', '21', '25', '29']\n",
            "train_data:  [('pianoroll', (54320, 128, 88)), ('tonal_centroid', (54320, 128, 6)), ('len', (54320,)), ('label', (54320, 128)), ('root', (54320, 128)), ('tquality', (54320, 128)), ('tchord', (54320, 128))]\n",
            "test_data:  [('pianoroll', (294, 128, 88)), ('tonal_centroid', (294, 128, 6)), ('len', (294,)), ('label', (294, 128)), ('root', (294, 128)), ('tquality', (294, 128)), ('tchord', (294, 128))]\n",
            "label fields: [('op', '<U10'), ('onset', '<f8'), ('key', '<U10'), ('degree1', '<U10'), ('degree2', '<U10'), ('quality', '<U10'), ('inversion', '<i8'), ('rchord', '<U10'), ('root', '<U10'), ('tquality', '<U10'), ('chord_change', '<i8')]\n",
            "load_data_symbol train_data size: (54320, 128)\n",
            "data torch.Size([3840128, 88])\n",
            "label torch.Size([3840128])\n",
            "data torch.Size([37632, 88])\n",
            "label torch.Size([37632])\n",
            "Size of train dataset:  3840128\n",
            "Size of val dataset:  37632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9CkJV5MthTU"
      },
      "source": [
        "#play example from BPS dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=8, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxlEUDlC_5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1fb4a158-c543-4005-a7c5-6692d5c6bcf9"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i,ba in enumerate(dataset[\"val\"]): \n",
        "    ##print(ba[0].shape)\n",
        "    if i < index: \n",
        "        continue\n",
        "    listen.append(tr(ba[0][:,None,None,:]))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll)\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-477cecd85049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#lets play a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inverse_norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlisten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eViZSBMro3iZ"
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Architecture Flags\n",
        "    parser.add_argument('--intermediate_dim', type=int, default=48)\n",
        "    parser.add_argument('--stripe_dim', type=int, default=24)\n",
        "    parser.add_argument('--num_stripes', type=int, default=48)\n",
        "    parser.add_argument('--num_active_neurons', type=int, default=24)\n",
        "    parser.add_argument('--num_active_stripes', type=int, default=5)\n",
        "    parser.add_argument('--layer_sparsity_mode', type=str, default='lifetime')  # Set to none, ordinary, boosted, or lifetime.\n",
        "    parser.add_argument('--stripe_sparsity_mode', type=str, default='routing')  # Set to none, ordinary, or routing.\n",
        "    parser.add_argument('--distort_prob', type=float, default=.01)  # Probability of stripe sparsity mask bits randomly flipping.\n",
        "    parser.add_argument('--distort_prob_decay', type=float, default=.025)  # Lowers distort_prob by this amount every epoch.\n",
        "\n",
        "    # Boosting Flags - Only necessary when layer_sparsity_mode is set to boosted.\n",
        "    parser.add_argument('--alpha', type=float, default=.8)\n",
        "    parser.add_argument('--beta', type=float, default=1.2)\n",
        "\n",
        "\n",
        "    # Routing Flags - Only necessary when stripe_sparsity_mode is set to routing.\n",
        "    parser.add_argument('--routing_l1_regularization', type=float, default=0.1)\n",
        "    parser.add_argument('--log_average_routing_scores', type=bool, default=True)\n",
        "\n",
        "    # Lifetime Stripe Flag - Only necessary when stripe_sparsity_mode is set to lifetime.\n",
        "    # Within a stripe, this specifies the proportion of samples that may activate the stripe.\n",
        "    parser.add_argument('--active_stripes_per_batch', type=float, default=7)\n",
        "\n",
        "    # Training Flags\n",
        "    parser.add_argument('--lr', type=float, default=.0001)\n",
        "    parser.add_argument('--momentum', type=float, default=.1)\n",
        "    parser.add_argument('--num_epochs', type=int, default=8)\n",
        "    parser.add_argument('--batch_size', type=int, default=8)\n",
        "    parser.add_argument('--data_path', type=str, default='data.csv')\n",
        "    parser.add_argument('--log_path', type=str, default='logs')\n",
        "    parser.add_argument('--log_class_specific_losses', type=bool, default=False)\n",
        "    parser.add_argument('--log_average_activations', type=bool, default=True)\n",
        "    parser.add_argument('--use_cuda_if_available', type=bool, default=True)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return vars(args)\n",
        "args = get_args()\n",
        "print(args[\"intermediate_dim\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEMdCVNo3qc"
      },
      "source": [
        "from ModularSparseAutoencoder.model import Net\n",
        "from ModularSparseAutoencoder.train import train\n",
        "\n",
        "\n",
        "num_stripes = args['num_stripes']\n",
        "num_epochs = args['num_epochs']\n",
        "batch_size = args['batch_size']\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "net = Net(args['intermediate_dim'],\n",
        "          args['stripe_dim'],\n",
        "          args['num_stripes'],\n",
        "          args['num_active_neurons'],\n",
        "          args['num_active_stripes'],\n",
        "          args['layer_sparsity_mode'],\n",
        "          args['stripe_sparsity_mode'],\n",
        "          args['distort_prob'],\n",
        "          args['alpha'],\n",
        "          args['beta'],\n",
        "          args['active_stripes_per_batch'],\n",
        "          device).double().to(device)\n",
        "\n",
        "# net.load_state_dict(torch.load(\"/content/net.pth\"))\n",
        "\n",
        "criterion = nn.MSELoss().to(device)\n",
        "# optimizer = optim.SGD(net.parameters(),\n",
        "#                       lr=args['lr'],\n",
        "#                       momentum=args['momentum'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'],  \n",
        "                       betas=(0.8, 0.999), eps=1e-08, weight_decay=0, amsgrad=True) ##this has weight decay just like you implemented\n",
        "\n",
        "timestamp = str(datetime.datetime.now()).replace(' ', '_')\n",
        "root_path = os.path.join(args['log_path'],\n",
        "                          args['layer_sparsity_mode'],\n",
        "                          args['stripe_sparsity_mode'],\n",
        "                          timestamp)\n",
        "print(f'Logging results to path:  {root_path}')\n",
        "\n",
        "distort_prob_decay = args['distort_prob_decay']\n",
        "routing_l1_regularization = (args['routing_l1_regularization'] if args['stripe_sparsity_mode'] == 'routing' else 0)\n",
        "log_class_specific_losses = args['log_class_specific_losses']\n",
        "should_log_average_routing_scores = (\n",
        "            args['stripe_sparsity_mode'] == 'routing' and args['log_average_routing_scores'])\n",
        "\n",
        "train(net,\n",
        "      criterion,\n",
        "      optimizer,\n",
        "      root_path,\n",
        "      dataset,\n",
        "      num_stripes,\n",
        "      num_epochs,\n",
        "      distort_prob_decay,\n",
        "      routing_l1_regularization,\n",
        "      log_class_specific_losses,\n",
        "      should_log_average_routing_scores)\n",
        "\n",
        "# if args['log_average_activations']:\n",
        "#     average_activations_path = os.path.join(root_path, 'average_activations.json')\n",
        "#     with open(average_activations_path, 'w') as f:\n",
        "#         average_activations = net.get_average_activations(X_test, Y_test, device=device).tolist()\n",
        "#         f.write(json.dumps(average_activations))\n",
        "\n",
        "flag_values_path = os.path.join(root_path, 'experiment_config.json')\n",
        "with open(flag_values_path, 'w') as f:\n",
        "    f.write(json.dumps(args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MpTvdvFFksg"
      },
      "source": [
        "#evaluate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0cjGZwwFj1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "22fb0dad-5fa7-4a15-d379-0b4a7f6b330c"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i, ba in enumerate(dataset[\"val\"]): \n",
        "    if i < index: \n",
        "        continue\n",
        "    x_var = ba[0].to(net.device)\n",
        "    code = net.code(x_var)\n",
        "    print(\"xvar shape \" , x_var.shape)\n",
        "    print(\"Code shape \" , code.shape)\n",
        "    xpred_var = net(x_var)[:,None,None,:]\n",
        "    listen.append(tr(xpred_var))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        piano_roll = (piano_roll>=1) \n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll.cpu().detach().numpy())\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f1a10074e48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#lets play a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inverse_norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlisten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mge7u8T0hoZE"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mk3ID7qF3Ss"
      },
      "source": [
        "\n",
        "Language Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on training a sequence-to-sequence model that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`__.\n",
        "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
        "to be superior in quality for many sequence-to-sequence tasks while being more\n",
        "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
        "mechanism (implemented as\n",
        "`nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`__)\n",
        "to draw global dependencies between input and output. The ``nn.Transformer``\n",
        "module is highly modularized such that a single component (e.g.,\n",
        "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__)\n",
        "can be easily adapted/composed.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUQ26bG3F3Su"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp9P4AQ5F3Su"
      },
      "source": [
        "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
        "Along with the input sequence, a square attention mask is required because the\n",
        "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
        "the earlier positions in the sequence. For the language modeling task, any\n",
        "tokens on the future positions should be masked. To produce a probability\n",
        "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
        "model is passed through a linear layer followed by a log-softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SXBpphnF3Sv"
      },
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Ddauu_F3Sw"
      },
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqEvIskYF3Sw"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqL1lWf0F3Sx"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AED7SZjYF3Sy"
      },
      "source": [
        "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
        "\n",
        "Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
        "into ``batch_size`` columns. If the data does not divide evenly into\n",
        "``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
        "the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
        "divide the alphabet into 4 sequences of length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that\n",
        "the model treats each column independently; for example, the dependence of\n",
        "``G`` and ``F`` can not be learned in the example above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "0QgXvp1_F3Sy",
        "outputId": "8621b1a5-3132-4628-fd14-dadccfa026e4"
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>']) \n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ed87b4b7f6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0meval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape [seq_len, batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ed87b4b7f6ec>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(data, bsz)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3x05FYnF3Sz"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymCBUT7F3Sz"
      },
      "source": [
        "``get_batch()`` generates a pair of input-target sequences for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv6jWIYOF3S0"
      },
      "source": [
        "bptt = 88\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfnl4czlF3S0"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edPrbL4XF3S1"
      },
      "source": [
        "The model hyperparameters are defined below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF1zU8CEF3S1",
        "outputId": "652bb474-2379-40e6-c19d-5ff5641721d3"
      },
      "source": [
        "ntokens = 25  # size of vocabulary\n",
        "print(ntokens)\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEblQn5TF3S1"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi0jeh4JF3S1"
      },
      "source": [
        "We use `CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__\n",
        "with the `SGD <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`__\n",
        "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
        "5.0 and follows a `StepLR <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html>`__\n",
        "schedule. During training, we use `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>`__\n",
        "to prevent gradients from exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMomjQDDF3S1"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "lr = 1e-2  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module, dataset) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(88).to(device)\n",
        "    bptt = 16\n",
        "    num_batches = 2000\n",
        "    for i, batch in enumerate(dataset[\"train\"]):\n",
        "        # data, targets = get_batch(train_data, i)\n",
        "        batch_size = int(batch[0].shape[0])\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        if i==2598:\n",
        "            break\n",
        "        targets = batch[1].repeat_interleave(88).long().to(device)\n",
        "        x = batch[0].long().to(device).T\n",
        "        output = model(x, src_mask)\n",
        "        # print(\"output shape\", output.view(-1, ntokens).shape)\n",
        "        # print(\"targets shape\", targets.shape)\n",
        "        # print(\"targets ex\", targets)\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "          lr = scheduler.get_last_lr()[0]\n",
        "          ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "          cur_loss = total_loss / log_interval\n",
        "          ppl = math.exp(cur_loss)\n",
        "          print(f'| epoch {epoch:3d} | {i:5d}/{num_batches:5d} batches | '\n",
        "                f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "          total_loss = 0\n",
        "          start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    bptt = 16\n",
        "    src_mask = generate_square_subsequent_mask(88).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(eval_data[\"val\"]):\n",
        "            batch_size = int(batch[0].shape[0])\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            targets = batch[1].repeat_interleave(88).long().to(device)\n",
        "            x = batch[0].long().to(device).T\n",
        "            output = model(x, src_mask)            \n",
        "            output_flat = output.view(-1, 25)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "            if i>17:\n",
        "              break\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfLEsm6AF3S2"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrGzXeDCF3S2",
        "outputId": "ec43e378-53e8-4bb7-ef17-af01cb13a62d"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # print(\"val data shape \", val_data.shape)\n",
        "    train(model, mydataset)\n",
        "    val_loss = evaluate(model, mydataset)\n",
        "    val_ppl = np.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2000 batches | lr 0.01000 | ms/batch  8.90 | loss  1.97 | ppl     7.14\n",
            "| epoch   1 |   400/ 2000 batches | lr 0.01000 | ms/batch  8.62 | loss  2.17 | ppl     8.78\n",
            "| epoch   1 |   600/ 2000 batches | lr 0.01000 | ms/batch  8.95 | loss  1.53 | ppl     4.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPJc4lhwF3S2"
      },
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVQvGy6F3S3"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}