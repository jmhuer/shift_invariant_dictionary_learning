{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "transformer_tutorial.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJ5sz2YF3Sp"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XGBgZ1gkyUdp",
        "outputId": "3e90840c-e653-4e09-d1cf-27355c6bb726"
      },
      "source": [
        "!pip install pretty_midi\n",
        "!git clone https://github.com/jmhuer/ModularSparseAutoencoder\n",
        "!git clone https://github.com/music-x-lab/POP909-Dataset\n",
        "!git clone https://github.com/jmhuer/HT\n",
        "!git clone https://github.com/Tsung-Ping/functional-harmony\n",
        "# %cd /content/POP909-Dataset/data_process\n",
        "!pip install libfmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.6 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=1f739b606a285929417de3e218409032d221bbe0aa0425ce549e57730bb2b118\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
            "Cloning into 'ModularSparseAutoencoder'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 12), reused 10 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n",
            "Cloning into 'POP909-Dataset'...\n",
            "remote: Enumerating objects: 9265, done.\u001b[K\n",
            "remote: Counting objects: 100% (9265/9265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8157/8157), done.\u001b[K\n",
            "remote: Total 9265 (delta 13), reused 9245 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9265/9265), 45.75 MiB | 24.36 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Cloning into 'HT'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 26 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n",
            "Cloning into 'functional-harmony'...\n",
            "remote: Enumerating objects: 460, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 460 (delta 13), reused 49 (delta 12), pack-reused 396\u001b[K\n",
            "Receiving objects: 100% (460/460), 2.50 MiB | 27.21 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "Collecting libfmp\n",
            "  Downloading libfmp-1.2.1-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.4.1)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.1.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (3.2.2)\n",
            "Collecting ipython<8.0.0,>=7.8.0\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788 kB 82.2 MB/s \n",
            "\u001b[?25hCollecting pysoundfile<1.0.0,>=0.9.0\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (1.19.5)\n",
            "Requirement already satisfied: numba<1.0.0,>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.51.2)\n",
            "Requirement already satisfied: librosa<1.0.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.8.1)\n",
            "Collecting music21<6.0.0,>=5.7.0\n",
            "  Downloading music21-5.7.2.tar.gz (18.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.5 MB 336 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pretty-midi<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from libfmp) (0.2.9)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (57.4.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 370 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.8.0->libfmp) (0.18.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.8.0->libfmp) (0.8.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.22.2.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (21.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (1.0.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0.0,>=0.8.0->libfmp) (0.10.3.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.1.0->libfmp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.1.0->libfmp) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1.0.0,>=0.51.0->libfmp) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->libfmp) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.8.0->libfmp) (0.7.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.23.0)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty-midi<1.0.0,>=0.2.0->libfmp) (1.2.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.8.0->libfmp) (0.2.5)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile<1.0.0,>=0.9.0->libfmp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile<1.0.0,>=0.9.0->libfmp) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<1.0.0,>=0.8.0->libfmp) (2.10)\n",
            "Building wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-5.7.2-py3-none-any.whl size=22024624 sha256=844107616eaf1dd2fa21ad8a3d843571ef976c6c8fe241f0f227c3b9b606c28c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cb/ae/fd264ebf1e9cf01c15576ee4c128f1bfd907a120c0a7a5b542\n",
            "Successfully built music21\n",
            "Installing collected packages: prompt-toolkit, pysoundfile, music21, ipython, libfmp\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: music21\n",
            "    Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.28.0 libfmp-1.2.1 music21-5.7.2 prompt-toolkit-3.0.20 pysoundfile-0.9.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3uwo-E8NFshn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab39520-a2ae-45aa-8376-a4e531398eb6"
      },
      "source": [
        "#@title Pytorch for DL\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "\n",
        "def get_model_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BA7MRaf3nrC"
      },
      "source": [
        "# This preprocesses data and creates pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dDuXiM626TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9721c8a6-3eac-4fb1-bc10-4a6f9400f9da"
      },
      "source": [
        "\n",
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "\n",
        "\n",
        "# Chord symbol recognition\n",
        "# train_BTC() # Bi-directional Transformer for Chord Recognition\n",
        "# train_CRNN() # Convolutional Recurrent Neural Network\n",
        "root_dict = {'C': 0, 'C+': 1, 'D': 2, 'D+': 3, 'E': 4, 'F': 5, 'F+': 6, 'G': 7, 'G+': 8, 'A': 9, 'A+': 10, 'B': 11, 'pad': 12}\n",
        "tquality_dict = {'M': 0, 'm': 1, 'O': 2, 'pad': 3}  # 'O' stands for 'others'\n",
        "n_chord_classes = 24 + 1  # 24 major-minor modes plus 1 others\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "hyperparameters = namedtuple('hyperparameters',\n",
        "                              ['dataset',\n",
        "                              'test_set_id',\n",
        "                              'graph_location',\n",
        "                              'n_root_classes',\n",
        "                              'n_tquality_classes',\n",
        "                              'n_chord_classes',\n",
        "                              'n_steps',\n",
        "                              'input_embed_size',\n",
        "                              'n_layers',\n",
        "                              'n_heads',\n",
        "                              'train_sequence_with_overlap',\n",
        "                              'initial_learning_rate',\n",
        "                              'drop',\n",
        "                              'n_batches',\n",
        "                              'n_training_steps',\n",
        "                              'n_in_succession',\n",
        "                              'annealing_rate'])\n",
        "\n",
        "hp = hyperparameters(dataset='/content/', # {'BPS_FH', 'Preludes'}\n",
        "                      test_set_id=1, # {1, 2, 3, 4}\n",
        "                      graph_location='model',\n",
        "                      n_root_classes=len(root_dict.keys()),\n",
        "                      n_tquality_classes=len(tquality_dict.keys()),\n",
        "                      n_chord_classes=n_chord_classes,\n",
        "                      n_steps=128,\n",
        "                      input_embed_size=128,\n",
        "                      n_layers=2,\n",
        "                      n_heads=4,\n",
        "                      train_sequence_with_overlap=True,\n",
        "                      initial_learning_rate=1e-4,\n",
        "                      drop=0.1,\n",
        "                      n_batches=40,\n",
        "                      n_training_steps=100000,\n",
        "                      n_in_succession=10,\n",
        "                      annealing_rate=1.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km3TD1zn3j3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c822fe-508d-400f-d04e-ea4e750b0238"
      },
      "source": [
        "from HT.BPS_FH_preprocessing import main\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: load note data ...\n",
            "lowest pitch = 24 highest pitch =  101\n",
            "Message: load chord labels...\n",
            "Message: get framewise labels ...\n",
            "max_length = 8482\n",
            "min_length = 872\n",
            "keys in corpus['op'] = dict_keys(['pianoroll', 'chromagram', 'start_time', 'label'])\n",
            "label fields =  [('op', '<U10'), ('onset', '<f8'), ('key', '<U10'), ('degree1', '<U10'), ('degree2', '<U10'), ('quality', '<U10'), ('inversion', '<i8'), ('rchord', '<U10'), ('root', '<U10'), ('tquality', '<U10'), ('chord_change', '<i8')]\n",
            "Running Message: augment data...\n",
            "keys in corpus_aug['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label'])\n",
            "Running Message: reshape data...\n",
            "keys in corpus_aug_reshape['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label', 'len'])\n",
            "sequence_len_non_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "sequence_len_overlaped = [17, 22, 23, 24, 30, 34, 36, 40, 44, 48, 52, 53, 66, 72, 80, 84, 86, 88, 100, 102, 104, 116, 118, 120, 128]\n",
            "Preprocessed data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrXxS30i3ygJ"
      },
      "source": [
        "# Make pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRQ7nWU1zIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c9213f-1f1a-4a07-c6a3-853c2d17cc54"
      },
      "source": [
        "# !pip install tensorflow-gpu==1.15\n",
        "from HT.chord_symbol_recognition import train_HT\n",
        "from collections import Counter, namedtuple\n",
        "from HT.BPS_FH_preprocessing import main\n",
        "from HT.chord_symbol_recognition import load_data_symbol\n",
        "#\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "# import utils\n",
        "import pretty_midi \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "class BPS(Dataset):\n",
        "    def __init__(self, data):\n",
        "        \"todo: transpose\"\n",
        "        self.all_data = []\n",
        "        temp = []\n",
        "        self.label = []\n",
        "        temp2 = []\n",
        "        self.chord_change = []\n",
        "        temp3 = []      \n",
        "        i = 0 \n",
        "        for pi,chord, cc in zip(data[\"pianoroll\"],data[\"tchord\"],data['label']['chord_change']):\n",
        "            temp.append(torch.tensor(pi).double())\n",
        "            temp2.append(torch.tensor(chord).double())\n",
        "            temp3.append(torch.tensor(cc).double())\n",
        "            i+=1\n",
        "            if i > 30000: break\n",
        "        self.all_data = torch.cat(temp)\n",
        "        self.label = torch.cat(temp2)\n",
        "        self.chord_change = torch.cat(temp3)\n",
        "        print(\"data\" , self.all_data.shape)\n",
        "        print(\"label\" , self.label.shape)\n",
        "        ##time.sleep(120)\n",
        "        self.transform = self.make_transform()\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "    def __getitem__(self, idx):\n",
        "        piano_roll_slice = self.all_data[idx,:]\n",
        "        label_slice = self.label[idx]\n",
        "        chord_change_slice = self.chord_change[idx]\n",
        "        tranformed_piano_roll_slice = self.transform[\"norm\"](piano_roll_slice[None][None])[0,0,:]\n",
        "        #print(tranformed_piano_roll_slice.shape)\n",
        "        #print(\"label_slice\", label_slice)\n",
        "        return tranformed_piano_roll_slice, label_slice, chord_change_slice\n",
        "    def make_transform(self):\n",
        "        mean = self.all_data.mean()\n",
        "        std = self.all_data.std() * 2\n",
        "        # print(\"mean \", mean.shape)\n",
        "        # print(\"std \", std.shape)\n",
        "        tensor_transform = {\n",
        "          'norm':\n",
        "              transforms.Compose([\n",
        "                  transforms.Normalize([mean], [std])  # Imagenet standards\n",
        "              ]),\n",
        "          \"inverse_norm\":\n",
        "                transforms.Normalize(\n",
        "                  mean= [-m/s for m, s in zip([mean], [std])],\n",
        "                  std= [1/s for s in [std]]\n",
        "                )\n",
        "            }\n",
        "        return tensor_transform\n",
        "\n",
        "\n",
        "def load_data(train_data, test_data,  num_workers=0, batch_size=16, random_seed = 40):\n",
        "    '''\n",
        "    this data loading proccedure assumes dataset/train/ dataset/val/ folders\n",
        "    also assumes transform dictionary with train and val\n",
        "    '''\n",
        "    dataset_train = BPS(train_data) \n",
        "    dataset_val = BPS(test_data) \n",
        "\n",
        "    print(\"Size of train dataset: \",len(dataset_train))\n",
        "    print(\"Size of val dataset: \",len(dataset_val))\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(dataset_train, batch_size=batch_size, shuffle=False, drop_last=True),\n",
        "        'val': DataLoader(dataset_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    }\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "\n",
        "##only tensor transforms\n",
        "\n",
        "\n",
        "\n",
        "train_data, test_data = load_data_symbol(dir=hp.dataset + 'BPS_FH_preprocessed_data_MIREX_Mm.pickle', test_set_id=hp.test_set_id, sequence_with_overlap=hp.train_sequence_with_overlap)\n",
        "print(\"load_data_symbol train_data size: {}\".format(train_data[\"tchord\"].shape))\n",
        "\n",
        "\n",
        "\n",
        "mydataset = load_data(train_data, test_data, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load chord symbol data...\n",
            "test_set_id = 1\n",
            "keys in corpus_aug_reshape['shift_id']['op'] = dict_keys(['pianoroll', 'tonal_centroid', 'start_time', 'label', 'len'])\n",
            "shift_list = ['shift_-1', 'shift_-2', 'shift_-3', 'shift_0', 'shift_1', 'shift_2', 'shift_3', 'shift_4', 'shift_5', 'shift_6']\n",
            "train_op_list = ['2', '3', '4', '6', '7', '8', '10', '11', '12', '14', '15', '16', '18', '19', '20', '22', '23', '24', '26', '27', '28', '30', '31', '32']\n",
            "test_op_list = ['1', '5', '9', '13', '17', '21', '25', '29']\n",
            "train_data:  [('pianoroll', (54320, 128, 88)), ('tonal_centroid', (54320, 128, 6)), ('len', (54320,)), ('label', (54320, 128)), ('root', (54320, 128)), ('tquality', (54320, 128)), ('tchord', (54320, 128))]\n",
            "test_data:  [('pianoroll', (294, 128, 88)), ('tonal_centroid', (294, 128, 6)), ('len', (294,)), ('label', (294, 128)), ('root', (294, 128)), ('tquality', (294, 128)), ('tchord', (294, 128))]\n",
            "label fields: [('op', '<U10'), ('onset', '<f8'), ('key', '<U10'), ('degree1', '<U10'), ('degree2', '<U10'), ('quality', '<U10'), ('inversion', '<i8'), ('rchord', '<U10'), ('root', '<U10'), ('tquality', '<U10'), ('chord_change', '<i8')]\n",
            "load_data_symbol train_data size: (54320, 128)\n",
            "data torch.Size([3840128, 88])\n",
            "label torch.Size([3840128])\n",
            "data torch.Size([37632, 88])\n",
            "label torch.Size([37632])\n",
            "Size of train dataset:  3840128\n",
            "Size of val dataset:  37632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9CkJV5MthTU"
      },
      "source": [
        "#play example from BPS dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow3sdkl5Xdy"
      },
      "source": [
        "\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs=8, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxlEUDlC_5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1fb4a158-c543-4005-a7c5-6692d5c6bcf9"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i,ba in enumerate(dataset[\"val\"]): \n",
        "    ##print(ba[0].shape)\n",
        "    if i < index: \n",
        "        continue\n",
        "    listen.append(tr(ba[0][:,None,None,:]))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll)\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-477cecd85049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#lets play a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inverse_norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlisten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eViZSBMro3iZ"
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Architecture Flags\n",
        "    parser.add_argument('--intermediate_dim', type=int, default=48)\n",
        "    parser.add_argument('--stripe_dim', type=int, default=24)\n",
        "    parser.add_argument('--num_stripes', type=int, default=48)\n",
        "    parser.add_argument('--num_active_neurons', type=int, default=24)\n",
        "    parser.add_argument('--num_active_stripes', type=int, default=5)\n",
        "    parser.add_argument('--layer_sparsity_mode', type=str, default='lifetime')  # Set to none, ordinary, boosted, or lifetime.\n",
        "    parser.add_argument('--stripe_sparsity_mode', type=str, default='routing')  # Set to none, ordinary, or routing.\n",
        "    parser.add_argument('--distort_prob', type=float, default=.01)  # Probability of stripe sparsity mask bits randomly flipping.\n",
        "    parser.add_argument('--distort_prob_decay', type=float, default=.025)  # Lowers distort_prob by this amount every epoch.\n",
        "\n",
        "    # Boosting Flags - Only necessary when layer_sparsity_mode is set to boosted.\n",
        "    parser.add_argument('--alpha', type=float, default=.8)\n",
        "    parser.add_argument('--beta', type=float, default=1.2)\n",
        "\n",
        "\n",
        "    # Routing Flags - Only necessary when stripe_sparsity_mode is set to routing.\n",
        "    parser.add_argument('--routing_l1_regularization', type=float, default=0.1)\n",
        "    parser.add_argument('--log_average_routing_scores', type=bool, default=True)\n",
        "\n",
        "    # Lifetime Stripe Flag - Only necessary when stripe_sparsity_mode is set to lifetime.\n",
        "    # Within a stripe, this specifies the proportion of samples that may activate the stripe.\n",
        "    parser.add_argument('--active_stripes_per_batch', type=float, default=7)\n",
        "\n",
        "    # Training Flags\n",
        "    parser.add_argument('--lr', type=float, default=.0001)\n",
        "    parser.add_argument('--momentum', type=float, default=.1)\n",
        "    parser.add_argument('--num_epochs', type=int, default=8)\n",
        "    parser.add_argument('--batch_size', type=int, default=8)\n",
        "    parser.add_argument('--data_path', type=str, default='data.csv')\n",
        "    parser.add_argument('--log_path', type=str, default='logs')\n",
        "    parser.add_argument('--log_class_specific_losses', type=bool, default=False)\n",
        "    parser.add_argument('--log_average_activations', type=bool, default=True)\n",
        "    parser.add_argument('--use_cuda_if_available', type=bool, default=True)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return vars(args)\n",
        "args = get_args()\n",
        "print(args[\"intermediate_dim\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEMdCVNo3qc"
      },
      "source": [
        "from ModularSparseAutoencoder.model import Net\n",
        "from ModularSparseAutoencoder.train import train\n",
        "\n",
        "\n",
        "num_stripes = args['num_stripes']\n",
        "num_epochs = args['num_epochs']\n",
        "batch_size = args['batch_size']\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "net = Net(args['intermediate_dim'],\n",
        "          args['stripe_dim'],\n",
        "          args['num_stripes'],\n",
        "          args['num_active_neurons'],\n",
        "          args['num_active_stripes'],\n",
        "          args['layer_sparsity_mode'],\n",
        "          args['stripe_sparsity_mode'],\n",
        "          args['distort_prob'],\n",
        "          args['alpha'],\n",
        "          args['beta'],\n",
        "          args['active_stripes_per_batch'],\n",
        "          device).double().to(device)\n",
        "\n",
        "# net.load_state_dict(torch.load(\"/content/net.pth\"))\n",
        "\n",
        "criterion = nn.MSELoss().to(device)\n",
        "# optimizer = optim.SGD(net.parameters(),\n",
        "#                       lr=args['lr'],\n",
        "#                       momentum=args['momentum'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'],  \n",
        "                       betas=(0.8, 0.999), eps=1e-08, weight_decay=0, amsgrad=True) ##this has weight decay just like you implemented\n",
        "\n",
        "timestamp = str(datetime.datetime.now()).replace(' ', '_')\n",
        "root_path = os.path.join(args['log_path'],\n",
        "                          args['layer_sparsity_mode'],\n",
        "                          args['stripe_sparsity_mode'],\n",
        "                          timestamp)\n",
        "print(f'Logging results to path:  {root_path}')\n",
        "\n",
        "distort_prob_decay = args['distort_prob_decay']\n",
        "routing_l1_regularization = (args['routing_l1_regularization'] if args['stripe_sparsity_mode'] == 'routing' else 0)\n",
        "log_class_specific_losses = args['log_class_specific_losses']\n",
        "should_log_average_routing_scores = (\n",
        "            args['stripe_sparsity_mode'] == 'routing' and args['log_average_routing_scores'])\n",
        "\n",
        "train(net,\n",
        "      criterion,\n",
        "      optimizer,\n",
        "      root_path,\n",
        "      dataset,\n",
        "      num_stripes,\n",
        "      num_epochs,\n",
        "      distort_prob_decay,\n",
        "      routing_l1_regularization,\n",
        "      log_class_specific_losses,\n",
        "      should_log_average_routing_scores)\n",
        "\n",
        "# if args['log_average_activations']:\n",
        "#     average_activations_path = os.path.join(root_path, 'average_activations.json')\n",
        "#     with open(average_activations_path, 'w') as f:\n",
        "#         average_activations = net.get_average_activations(X_test, Y_test, device=device).tolist()\n",
        "#         f.write(json.dumps(average_activations))\n",
        "\n",
        "flag_values_path = os.path.join(root_path, 'experiment_config.json')\n",
        "with open(flag_values_path, 'w') as f:\n",
        "    f.write(json.dumps(args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MpTvdvFFksg"
      },
      "source": [
        "#evaluate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0cjGZwwFj1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "22fb0dad-5fa7-4a15-d379-0b4a7f6b330c"
      },
      "source": [
        "import IPython.display\n",
        "index = 45 ## 44 66 & 0 & 1 500 omg 1021\n",
        "\n",
        "#lets play a batch \n",
        "tr = dataset[\"val\"].dataset.transform[\"inverse_norm\"]\n",
        "listen = []\n",
        "for i, ba in enumerate(dataset[\"val\"]): \n",
        "    if i < index: \n",
        "        continue\n",
        "    x_var = ba[0].to(net.device)\n",
        "    code = net.code(x_var)\n",
        "    print(\"xvar shape \" , x_var.shape)\n",
        "    print(\"Code shape \" , code.shape)\n",
        "    xpred_var = net(x_var)[:,None,None,:]\n",
        "    listen.append(tr(xpred_var))\n",
        "    if len(listen)==10: \n",
        "        piano_roll = torch.cat(listen)[:,0,0,:].T\n",
        "        piano_roll = (piano_roll>=1) \n",
        "        break\n",
        "\n",
        "def pad88to128(piano_roll):\n",
        "    arr = np.zeros((128,int(piano_roll.shape[1])))\n",
        "    pad = (128 - 88)//2\n",
        "    arr[pad:(128-pad),0:arr.shape[1]] = piano_roll\n",
        "    return arr\n",
        "\n",
        "arr = pad88to128(piano_roll.cpu().detach().numpy())\n",
        "pm = piano_roll_to_pretty_midi(arr)\n",
        "IPython.display.Audio(pm.synthesize(fs=16000), rate=16000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f1a10074e48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#lets play a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inverse_norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlisten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mge7u8T0hoZE"
      },
      "source": [
        "import libfmp.c1\n",
        "score = libfmp.c1.midi_to_list(pm)\n",
        "\n",
        "libfmp.c1.visualize_piano_roll(score, figsize=(8, 3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z8mismuC56F"
      },
      "source": [
        "#ibm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5ZCe9vFC5cK",
        "outputId": "35fce639-82d0-4664-8dac-88440a2fa77a"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA9mjGZ4C5gE",
        "outputId": "938555cb-6c74-4c76-b7c0-34277f403a76"
      },
      "source": [
        "import condacolab\n",
        "condacolab.check()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrPDMTx0DCXV",
        "outputId": "f065d6e8-3a98-4efd-d3b8-ff0a294e6319"
      },
      "source": [
        "!conda env create -n base -f TabFormer/setup.yml"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
            "\n",
            "CondaValueError: prefix already exists: /usr/local\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f79hs9BmHNED",
        "outputId": "e90d8c17-263a-415e-bebb-03d0ea2c414f"
      },
      "source": [
        "!pip install transformers==3.2.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.2.0\n",
            "  Using cached transformers-3.2.0-py3-none-any.whl (1.0 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (1.21.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (3.3.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Using cached tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (4.59.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from transformers==3.2.0) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->transformers==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->transformers==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->transformers==3.2.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->transformers==3.2.0) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->transformers==3.2.0) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers==3.2.0) (8.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers==3.2.0) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from click->sacremoses->transformers==3.2.0) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.2.0) (3.10.0.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.11.3\n",
            "    Uninstalling transformers-4.11.3:\n",
            "      Successfully uninstalled transformers-4.11.3\n",
            "Successfully installed tokenizers-0.8.1rc2 transformers-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssKP6lnhKjqB"
      },
      "source": [
        "!tar -xf  /content/credit_card/transactions.tgz"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk-uiDwYEi8V",
        "outputId": "5132a5ae-5e31-4da0-caaa-1ae00b44af20"
      },
      "source": [
        ""
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOftvz1pP8z7"
      },
      "source": [
        "#RUN TRANFORMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuwlKc-0ca12"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dN7i2fSaDjj7",
        "outputId": "3a5d0011-206a-4c48-d29c-0df326d6cfca"
      },
      "source": [
        "from TabFormer.args import define_main_parser\n",
        "from os.path import join\n",
        "from os import makedirs\n",
        "from TabFormer.main import main\n",
        "\n",
        "parser = define_main_parser()\n",
        "opts = parser.parse_args(args=[])\n",
        "\n",
        "opts.log_dir = join(opts.output_dir, \"logs\")\n",
        "makedirs(opts.output_dir, exist_ok=True)\n",
        "makedirs(opts.log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "#my args \n",
        "opts.mlm = True\n",
        "\n",
        "if opts.mlm and opts.lm_type == \"gpt2\":\n",
        "    raise Exception(\"Error: GPT2 doesn't need '--mlm' option. Please re-run with this flag removed.\")\n",
        "\n",
        "if not opts.mlm and opts.lm_type == \"bert\":\n",
        "    raise Exception(\"Error: Bert needs '--mlm' option. Please re-run with this flag included.\")\n",
        "\n",
        "main(opts)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   read data : (200, 15)\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   /content/TabFormer/data/credit_card/card_transaction.v1.csv is read.\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   nan resolution.\n",
            "10/16/2021 19:40:51 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   label-fit-transform.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 1954.48it/s]\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   timestamp fit transform\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   timestamp quant transform\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   amount quant transform\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   writing cached csv to /content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.encoded.csv\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   writing to file /content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.encoded.csv\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   writing cached encoder fit to /content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.encoder_fit.pkl\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   total columns: ['User', 'Card', 'Timestamp', 'Amount', 'Use Chip', 'Merchant Name', 'Merchant City', 'Merchant State', 'Zip', 'MCC', 'Errors?', 'Is Fraud?']\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   total vocabulary size: 121\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : User, vocab size : 1\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Card, vocab size : 1\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Timestamp, vocab size : 10\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Amount, vocab size : 10\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Use Chip, vocab size : 2\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Merchant Name, vocab size : 41\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Merchant City, vocab size : 8\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Merchant State, vocab size : 3\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Zip, vocab size : 9\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : MCC, vocab size : 25\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Errors?, vocab size : 3\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : Is Fraud?, vocab size : 1\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   column : SPECIAL, vocab size : 7\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   preparing user level data...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.05it/s]\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   creating transaction samples with vocab\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 411.33it/s]\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   ncols: 12\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   no of samples 39\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.dataset.card -   saving vocab at checkpoints/vocab.nb\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.main -   # lengths: train [23]  valid [8]  test [8]\n",
            "10/16/2021 19:40:51 - INFO - TabFormer.main -   # lengths: train [0.59]  valid [0.21]  test [0.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rain_dataset  120\n",
            "train_dataset  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/16/2021 19:43:04 - INFO - TabFormer.main -   model initiated: <class 'TabFormer.models.modules.TabFormerHierarchicalLM'>\n",
            "10/16/2021 19:43:04 - INFO - TabFormer.main -   collactor class: TransDataCollatorForLanguageModeling\n",
            "/usr/local/lib/python3.7/site-packages/transformers/training_args.py:291: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9e81b1765a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: Bert needs '--mlm' option. Please re-run with this flag included.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TabFormer/main.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, tb_writer, optimizers, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mdefault_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_data_collator\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 15.78 GiB total capacity; 13.96 GiB already allocated; 286.75 MiB free; 13.96 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdudlVFwhcPU",
        "outputId": "35e7e78c-31b0-42eb-8a02-864649a52f65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers --upgrade"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/site-packages (3.2.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/site-packages (from transformers) (4.59.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/site-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from transformers) (3.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from transformers) (1.21.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers) (2021.10.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc2\n",
            "    Uninstalling tokenizers-0.8.1rc2:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.2.0\n",
            "    Uninstalling transformers-3.2.0:\n",
            "      Successfully uninstalled transformers-3.2.0\n",
            "Successfully installed tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPdwkFBHdzXr",
        "outputId": "7399a73d-19bc-42b7-9cc3-9b5d448c3481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!zip -r myTabFormer.zip /content/TabFormer"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/TabFormer/ (stored 0%)\n",
            "updating: content/TabFormer/README.md (deflated 51%)\n",
            "updating: content/TabFormer/dataset/ (stored 0%)\n",
            "updating: content/TabFormer/dataset/datacollator.py (deflated 65%)\n",
            "updating: content/TabFormer/dataset/vocab.py (deflated 73%)\n",
            "updating: content/TabFormer/dataset/card.py (deflated 73%)\n",
            "updating: content/TabFormer/dataset/prsa.py (deflated 69%)\n",
            "updating: content/TabFormer/dataset/__init__.py (deflated 8%)\n",
            "updating: content/TabFormer/dataset/__pycache__/ (stored 0%)\n",
            "updating: content/TabFormer/dataset/__pycache__/vocab.cpython-37.pyc (deflated 49%)\n",
            "updating: content/TabFormer/dataset/__pycache__/datacollator.cpython-37.pyc (deflated 43%)\n",
            "updating: content/TabFormer/dataset/__pycache__/card.cpython-37.pyc (deflated 49%)\n",
            "updating: content/TabFormer/dataset/__pycache__/__init__.cpython-37.pyc (deflated 17%)\n",
            "updating: content/TabFormer/dataset/__pycache__/prsa.cpython-37.pyc (deflated 44%)\n",
            "updating: content/TabFormer/.git/ (stored 0%)\n",
            "updating: content/TabFormer/.git/branches/ (stored 0%)\n",
            "updating: content/TabFormer/.git/index (deflated 45%)\n",
            "updating: content/TabFormer/.git/refs/ (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/heads/ (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/heads/main (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/tags/ (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/remotes/ (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/remotes/origin/ (stored 0%)\n",
            "updating: content/TabFormer/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "updating: content/TabFormer/.git/packed-refs (deflated 32%)\n",
            "updating: content/TabFormer/.git/info/ (stored 0%)\n",
            "updating: content/TabFormer/.git/info/exclude (deflated 28%)\n",
            "updating: content/TabFormer/.git/config (deflated 32%)\n",
            "updating: content/TabFormer/.git/hooks/ (stored 0%)\n",
            "updating: content/TabFormer/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "updating: content/TabFormer/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "updating: content/TabFormer/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "updating: content/TabFormer/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "updating: content/TabFormer/.git/hooks/post-update.sample (deflated 27%)\n",
            "updating: content/TabFormer/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "updating: content/TabFormer/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "updating: content/TabFormer/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "updating: content/TabFormer/.git/hooks/pre-push.sample (deflated 50%)\n",
            "updating: content/TabFormer/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "updating: content/TabFormer/.git/hooks/update.sample (deflated 68%)\n",
            "updating: content/TabFormer/.git/logs/ (stored 0%)\n",
            "updating: content/TabFormer/.git/logs/refs/ (stored 0%)\n",
            "updating: content/TabFormer/.git/logs/refs/heads/ (stored 0%)\n",
            "updating: content/TabFormer/.git/logs/refs/heads/main (deflated 26%)\n",
            "updating: content/TabFormer/.git/logs/refs/remotes/ (stored 0%)\n",
            "updating: content/TabFormer/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "updating: content/TabFormer/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
            "updating: content/TabFormer/.git/logs/HEAD (deflated 26%)\n",
            "updating: content/TabFormer/.git/objects/ (stored 0%)\n",
            "updating: content/TabFormer/.git/objects/pack/ (stored 0%)\n",
            "updating: content/TabFormer/.git/objects/pack/pack-a308e51db65533ab83755fb38f278cb939c08934.idx (deflated 20%)\n",
            "updating: content/TabFormer/.git/objects/pack/pack-a308e51db65533ab83755fb38f278cb939c08934.pack (deflated 0%)\n",
            "updating: content/TabFormer/.git/objects/info/ (stored 0%)\n",
            "updating: content/TabFormer/.git/HEAD (stored 0%)\n",
            "updating: content/TabFormer/.git/description (deflated 14%)\n",
            "updating: content/TabFormer/args.py (deflated 77%)\n",
            "updating: content/TabFormer/misc/ (stored 0%)\n",
            "updating: content/TabFormer/misc/utils.py (deflated 64%)\n",
            "updating: content/TabFormer/misc/__init__.py (stored 0%)\n",
            "updating: content/TabFormer/misc/__pycache__/ (stored 0%)\n",
            "updating: content/TabFormer/misc/__pycache__/utils.cpython-37.pyc (deflated 43%)\n",
            "updating: content/TabFormer/misc/__pycache__/__init__.cpython-37.pyc (deflated 25%)\n",
            "updating: content/TabFormer/misc/cc_trans_dataset.png (deflated 1%)\n",
            "updating: content/TabFormer/LICENSE (deflated 65%)\n",
            "updating: content/TabFormer/.gitattributes (deflated 12%)\n",
            "updating: content/TabFormer/setup.yml (deflated 38%)\n",
            "updating: content/TabFormer/models/ (stored 0%)\n",
            "updating: content/TabFormer/models/tabformer_tokenizer.py (deflated 54%)\n",
            "updating: content/TabFormer/models/custom_criterion.py (deflated 67%)\n",
            "updating: content/TabFormer/models/tabformer_bert.py (deflated 77%)\n",
            "updating: content/TabFormer/models/modules.py (deflated 77%)\n",
            "updating: content/TabFormer/models/tabformer_gpt2.py (deflated 67%)\n",
            "updating: content/TabFormer/models/hierarchical.py (deflated 75%)\n",
            "updating: content/TabFormer/models/__init__.py (stored 0%)\n",
            "updating: content/TabFormer/models/__pycache__/ (stored 0%)\n",
            "updating: content/TabFormer/models/__pycache__/modules.cpython-37.pyc (deflated 53%)\n",
            "updating: content/TabFormer/models/__pycache__/custom_criterion.cpython-37.pyc (deflated 37%)\n",
            "updating: content/TabFormer/models/__pycache__/tabformer_bert.cpython-37.pyc (deflated 55%)\n",
            "updating: content/TabFormer/models/__pycache__/tabformer_tokenizer.cpython-37.pyc (deflated 40%)\n",
            "updating: content/TabFormer/models/__pycache__/__init__.cpython-37.pyc (deflated 25%)\n",
            "updating: content/TabFormer/models/__pycache__/hierarchical.cpython-37.pyc (deflated 56%)\n",
            "updating: content/TabFormer/models/__pycache__/tabformer_gpt2.cpython-37.pyc (deflated 40%)\n",
            "updating: content/TabFormer/data/ (stored 0%)\n",
            "updating: content/TabFormer/data/credit_card/ (stored 0%)\n",
            "updating: content/TabFormer/data/credit_card/README.md (deflated 9%)\n",
            "updating: content/TabFormer/data/credit_card/preprocessed/ (stored 0%)\n",
            "updating: content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.user.pkl (deflated 93%)\n",
            "updating: content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.encoder_fit.pkl (deflated 27%)\n",
            "updating: content/TabFormer/data/credit_card/preprocessed/card_transaction.v1.encoded.csv (deflated 85%)\n",
            "updating: content/TabFormer/data/credit_card/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: content/TabFormer/data/credit_card/transactions.tgz (deflated 15%)\n",
            "updating: content/TabFormer/main.py (deflated 70%)\n",
            "updating: content/TabFormer/__pycache__/ (stored 0%)\n",
            "updating: content/TabFormer/__pycache__/main.cpython-37.pyc (deflated 40%)\n",
            "updating: content/TabFormer/__pycache__/args.cpython-37.pyc (deflated 46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mk3ID7qF3Ss"
      },
      "source": [
        "\n",
        "Language Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on training a sequence-to-sequence model that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`__.\n",
        "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
        "to be superior in quality for many sequence-to-sequence tasks while being more\n",
        "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
        "mechanism (implemented as\n",
        "`nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`__)\n",
        "to draw global dependencies between input and output. The ``nn.Transformer``\n",
        "module is highly modularized such that a single component (e.g.,\n",
        "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__)\n",
        "can be easily adapted/composed.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUQ26bG3F3Su"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp9P4AQ5F3Su"
      },
      "source": [
        "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
        "Along with the input sequence, a square attention mask is required because the\n",
        "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
        "the earlier positions in the sequence. For the language modeling task, any\n",
        "tokens on the future positions should be masked. To produce a probability\n",
        "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
        "model is passed through a linear layer followed by a log-softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SXBpphnF3Sv"
      },
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Ddauu_F3Sw"
      },
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqEvIskYF3Sw"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqL1lWf0F3Sx"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AED7SZjYF3Sy"
      },
      "source": [
        "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
        "\n",
        "Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
        "into ``batch_size`` columns. If the data does not divide evenly into\n",
        "``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
        "the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
        "divide the alphabet into 4 sequences of length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that\n",
        "the model treats each column independently; for example, the dependence of\n",
        "``G`` and ``F`` can not be learned in the example above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "0QgXvp1_F3Sy",
        "outputId": "8621b1a5-3132-4628-fd14-dadccfa026e4"
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>']) \n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ed87b4b7f6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0meval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape [seq_len, batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ed87b4b7f6ec>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(data, bsz)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3x05FYnF3Sz"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymCBUT7F3Sz"
      },
      "source": [
        "``get_batch()`` generates a pair of input-target sequences for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "weâ€™d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv6jWIYOF3S0"
      },
      "source": [
        "bptt = 88\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfnl4czlF3S0"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edPrbL4XF3S1"
      },
      "source": [
        "The model hyperparameters are defined below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF1zU8CEF3S1",
        "outputId": "6d2a71fc-829e-4d09-f54b-29874c34ddc4"
      },
      "source": [
        "ntokens = 25  # size of vocabulary\n",
        "print(ntokens)\n",
        "emsize = 20  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "print(ntokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEblQn5TF3S1"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi0jeh4JF3S1"
      },
      "source": [
        "We use `CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__\n",
        "with the `SGD <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`__\n",
        "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
        "5.0 and follows a `StepLR <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html>`__\n",
        "schedule. During training, we use `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>`__\n",
        "to prevent gradients from exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMomjQDDF3S1"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "lr = 1e-3 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module, dataset) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(88).to(device)\n",
        "    bptt = 16\n",
        "    num_batches = 2400\n",
        "    for i, batch in enumerate(dataset[\"train\"]):\n",
        "        # data, targets = get_batch(train_data, i)\n",
        "        batch_size = int(batch[0].shape[0])\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        if i==2598:\n",
        "            break\n",
        "        targets = batch[1].repeat_interleave(88).long().to(device)\n",
        "        x = batch[0].long().to(device).T\n",
        "        output = model(x, src_mask)\n",
        "        # print(\"output shape\", output.view(-1, ntokens).shape)\n",
        "        # print(\"targets shape\", targets.shape)\n",
        "        # print(\"targets ex\", targets)\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "          lr = scheduler.get_last_lr()[0]\n",
        "          ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "          cur_loss = total_loss / log_interval\n",
        "          ppl = math.exp(cur_loss)\n",
        "          print(f'| epoch {epoch:3d} | {i:5d}/{num_batches:5d} batches | '\n",
        "                f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "          total_loss = 0\n",
        "          start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    bptt = 16\n",
        "    src_mask = generate_square_subsequent_mask(88).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(eval_data[\"val\"]):\n",
        "            batch_size = int(batch[0].shape[0])\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            targets = batch[1].repeat_interleave(88).long().to(device)\n",
        "            x = batch[0].long().to(device).T\n",
        "            output = model(x, src_mask)            \n",
        "            loss = criterion(output.view(-1, ntokens), targets)\n",
        "            total_loss += loss.item()\n",
        "            if i==18:\n",
        "                return total_loss / (i+1)\n",
        "    return total_loss / (i+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfLEsm6AF3S2"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrGzXeDCF3S2",
        "outputId": "08f2dc9a-84ca-459e-fe77-7bcb9f62ffd3"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 30\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # print(\"val data shape \", val_data.shape)\n",
        "    train(model, mydataset)\n",
        "    val_loss = evaluate(model, mydataset)\n",
        "    val_ppl = np.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2400 batches | lr 0.00100 | ms/batch  8.75 | loss  3.32 | ppl    27.63\n",
            "| epoch   1 |   400/ 2400 batches | lr 0.00100 | ms/batch  8.70 | loss  3.22 | ppl    25.04\n",
            "| epoch   1 |   600/ 2400 batches | lr 0.00100 | ms/batch  8.52 | loss  3.06 | ppl    21.37\n",
            "| epoch   1 |   800/ 2400 batches | lr 0.00100 | ms/batch  8.67 | loss  3.16 | ppl    23.64\n",
            "| epoch   1 |  1000/ 2400 batches | lr 0.00100 | ms/batch  8.73 | loss  3.16 | ppl    23.67\n",
            "| epoch   1 |  1200/ 2400 batches | lr 0.00100 | ms/batch  8.36 | loss  3.38 | ppl    29.38\n",
            "| epoch   1 |  1400/ 2400 batches | lr 0.00100 | ms/batch  8.25 | loss  2.90 | ppl    18.22\n",
            "| epoch   1 |  1600/ 2400 batches | lr 0.00100 | ms/batch  8.59 | loss  2.91 | ppl    18.32\n",
            "| epoch   1 |  1800/ 2400 batches | lr 0.00100 | ms/batch  8.67 | loss  2.91 | ppl    18.34\n",
            "| epoch   1 |  2000/ 2400 batches | lr 0.00100 | ms/batch  8.04 | loss  3.37 | ppl    29.02\n",
            "| epoch   1 |  2200/ 2400 batches | lr 0.00100 | ms/batch  8.07 | loss  2.88 | ppl    17.84\n",
            "| epoch   1 |  2400/ 2400 batches | lr 0.00100 | ms/batch  8.01 | loss  2.62 | ppl    13.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.93s | valid loss  3.12 | valid ppl    22.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2400 batches | lr 0.00095 | ms/batch  8.16 | loss  2.77 | ppl    15.91\n",
            "| epoch   2 |   400/ 2400 batches | lr 0.00095 | ms/batch  8.40 | loss  2.85 | ppl    17.25\n",
            "| epoch   2 |   600/ 2400 batches | lr 0.00095 | ms/batch  8.36 | loss  2.34 | ppl    10.43\n",
            "| epoch   2 |   800/ 2400 batches | lr 0.00095 | ms/batch  8.32 | loss  2.93 | ppl    18.71\n",
            "| epoch   2 |  1000/ 2400 batches | lr 0.00095 | ms/batch  8.38 | loss  2.95 | ppl    19.05\n",
            "| epoch   2 |  1200/ 2400 batches | lr 0.00095 | ms/batch  8.59 | loss  3.38 | ppl    29.30\n",
            "| epoch   2 |  1400/ 2400 batches | lr 0.00095 | ms/batch  8.03 | loss  2.40 | ppl    11.04\n",
            "| epoch   2 |  1600/ 2400 batches | lr 0.00095 | ms/batch  7.94 | loss  2.47 | ppl    11.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPJc4lhwF3S2"
      },
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVQvGy6F3S3"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}